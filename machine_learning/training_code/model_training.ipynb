{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849ccb47",
   "metadata": {},
   "source": [
    "# Hedge Your Bets - AI-Powered Sports Betting Analysis\n",
    "\n",
    "## Model Training Pipeline\n",
    "\n",
    "This notebook implements the complete ML pipeline for predicting player prop outcomes and evaluating betting value.\n",
    "\n",
    "### Approach Overview:\n",
    "1. **Data Preprocessing**: Load and clean player/team weekly stats\n",
    "2. **Feature Engineering**: Create rolling averages, team context, game context\n",
    "3. **Model Training**: Per-position LightGBM quantile models\n",
    "4. **Evaluation**: Betting-relevant metrics and profit simulation\n",
    "\n",
    "## Important note for inference:\n",
    "\n",
    "Will need to collect player, player team, opponent team, and market from user. Then the backend will pulls the latest season data and constructs the exact features the model expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c748fa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.2.2\n",
      "LightGBM version: 4.6.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0adfca9",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "First, let's load and explore our datasets to understand the structure and data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf34e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory structure:\n",
      "Player weekly stats: ..\\datasets\\player_weekly_stats\n",
      "Team weekly stats: ..\\datasets\\team_season_stats\n",
      "Players metadata: ..\\datasets\\players.csv\n",
      "\n",
      "Found 26 player weekly stat files\n",
      "Found 26 team weekly stat files\n",
      "Years covered: ['1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024']\n"
     ]
    }
   ],
   "source": [
    "# Set up data paths\n",
    "data_dir = Path(\"../datasets\")\n",
    "player_weekly_dir = data_dir / \"player_weekly_stats\"\n",
    "team_weekly_dir = data_dir / \"team_season_stats\"\n",
    "players_file = data_dir / \"players.csv\"\n",
    "\n",
    "print(\"Data directory structure:\")\n",
    "print(f\"Player weekly stats: {player_weekly_dir}\")\n",
    "print(f\"Team weekly stats: {team_weekly_dir}\")\n",
    "print(f\"Players metadata: {players_file}\")\n",
    "\n",
    "# Check what files we have\n",
    "player_files = list(player_weekly_dir.glob(\"*.csv\"))\n",
    "team_files = list(team_weekly_dir.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"\\nFound {len(player_files)} player weekly stat files\")\n",
    "print(f\"Found {len(team_files)} team weekly stat files\")\n",
    "print(f\"Years covered: {sorted([f.stem.split('_')[-1] for f in player_files])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf14c660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player data sample (2024):\n",
      "Shape: (18981, 114)\n",
      "Columns: ['player_id', 'player_name', 'player_display_name', 'position', 'position_group', 'headshot_url', 'season', 'week', 'season_type', 'team', 'opponent_team', 'completions', 'attempts', 'passing_yards', 'passing_tds', 'passing_interceptions', 'sacks_suffered', 'sack_yards_lost', 'sack_fumbles', 'sack_fumbles_lost', 'passing_air_yards', 'passing_yards_after_catch', 'passing_first_downs', 'passing_epa', 'passing_cpoe', 'passing_2pt_conversions', 'pacr', 'carries', 'rushing_yards', 'rushing_tds', 'rushing_fumbles', 'rushing_fumbles_lost', 'rushing_first_downs', 'rushing_epa', 'rushing_2pt_conversions', 'receptions', 'targets', 'receiving_yards', 'receiving_tds', 'receiving_fumbles', 'receiving_fumbles_lost', 'receiving_air_yards', 'receiving_yards_after_catch', 'receiving_first_downs', 'receiving_epa', 'receiving_2pt_conversions', 'racr', 'target_share', 'air_yards_share', 'wopr', 'special_teams_tds', 'def_tackles_solo', 'def_tackles_with_assist', 'def_tackle_assists', 'def_tackles_for_loss', 'def_tackles_for_loss_yards', 'def_fumbles_forced', 'def_sacks', 'def_sack_yards', 'def_qb_hits', 'def_interceptions', 'def_interception_yards', 'def_pass_defended', 'def_tds', 'def_fumbles', 'def_safeties', 'misc_yards', 'fumble_recovery_own', 'fumble_recovery_yards_own', 'fumble_recovery_opp', 'fumble_recovery_yards_opp', 'fumble_recovery_tds', 'penalties', 'penalty_yards', 'punt_returns', 'punt_return_yards', 'kickoff_returns', 'kickoff_return_yards', 'fg_made', 'fg_att', 'fg_missed', 'fg_blocked', 'fg_long', 'fg_pct', 'fg_made_0_19', 'fg_made_20_29', 'fg_made_30_39', 'fg_made_40_49', 'fg_made_50_59', 'fg_made_60_', 'fg_missed_0_19', 'fg_missed_20_29', 'fg_missed_30_39', 'fg_missed_40_49', 'fg_missed_50_59', 'fg_missed_60_', 'fg_made_list', 'fg_missed_list', 'fg_blocked_list', 'fg_made_distance', 'fg_missed_distance', 'fg_blocked_distance', 'pat_made', 'pat_att', 'pat_missed', 'pat_blocked', 'pat_pct', 'gwfg_made', 'gwfg_att', 'gwfg_missed', 'gwfg_blocked', 'gwfg_distance', 'fantasy_points', 'fantasy_points_ppr']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>player_name</th>\n",
       "      <th>player_display_name</th>\n",
       "      <th>position</th>\n",
       "      <th>position_group</th>\n",
       "      <th>headshot_url</th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>season_type</th>\n",
       "      <th>team</th>\n",
       "      <th>...</th>\n",
       "      <th>pat_missed</th>\n",
       "      <th>pat_blocked</th>\n",
       "      <th>pat_pct</th>\n",
       "      <th>gwfg_made</th>\n",
       "      <th>gwfg_att</th>\n",
       "      <th>gwfg_missed</th>\n",
       "      <th>gwfg_blocked</th>\n",
       "      <th>gwfg_distance</th>\n",
       "      <th>fantasy_points</th>\n",
       "      <th>fantasy_points_ppr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-0023459</td>\n",
       "      <td>A.Rodgers</td>\n",
       "      <td>Aaron Rodgers</td>\n",
       "      <td>QB</td>\n",
       "      <td>QB</td>\n",
       "      <td>https://static.www.nfl.com/image/upload/f_auto...</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>REG</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.58</td>\n",
       "      <td>8.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-0023853</td>\n",
       "      <td>M.Prater</td>\n",
       "      <td>Matt Prater</td>\n",
       "      <td>K</td>\n",
       "      <td>SPEC</td>\n",
       "      <td>https://static.www.nfl.com/image/upload/f_auto...</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>REG</td>\n",
       "      <td>ARI</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-0025565</td>\n",
       "      <td>N.Folk</td>\n",
       "      <td>Nick Folk</td>\n",
       "      <td>K</td>\n",
       "      <td>SPEC</td>\n",
       "      <td>https://static.www.nfl.com/image/upload/f_auto...</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>REG</td>\n",
       "      <td>TEN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-0026190</td>\n",
       "      <td>C.Campbell</td>\n",
       "      <td>Calais Campbell</td>\n",
       "      <td>DE</td>\n",
       "      <td>DL</td>\n",
       "      <td>https://static.www.nfl.com/image/upload/f_auto...</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>REG</td>\n",
       "      <td>MIA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-0026498</td>\n",
       "      <td>M.Stafford</td>\n",
       "      <td>Matthew Stafford</td>\n",
       "      <td>QB</td>\n",
       "      <td>QB</td>\n",
       "      <td>https://static.www.nfl.com/image/upload/f_auto...</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>REG</td>\n",
       "      <td>LA</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.68</td>\n",
       "      <td>14.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    player_id player_name player_display_name position position_group  \\\n",
       "0  00-0023459   A.Rodgers       Aaron Rodgers       QB             QB   \n",
       "1  00-0023853    M.Prater         Matt Prater        K           SPEC   \n",
       "2  00-0025565      N.Folk           Nick Folk        K           SPEC   \n",
       "3  00-0026190  C.Campbell     Calais Campbell       DE             DL   \n",
       "4  00-0026498  M.Stafford    Matthew Stafford       QB             QB   \n",
       "\n",
       "                                        headshot_url  season  week  \\\n",
       "0  https://static.www.nfl.com/image/upload/f_auto...    2024     1   \n",
       "1  https://static.www.nfl.com/image/upload/f_auto...    2024     1   \n",
       "2  https://static.www.nfl.com/image/upload/f_auto...    2024     1   \n",
       "3  https://static.www.nfl.com/image/upload/f_auto...    2024     1   \n",
       "4  https://static.www.nfl.com/image/upload/f_auto...    2024     1   \n",
       "\n",
       "  season_type team  ... pat_missed  pat_blocked  pat_pct  gwfg_made  gwfg_att  \\\n",
       "0         REG  NYJ  ...          0            0      NaN          0         0   \n",
       "1         REG  ARI  ...          0            0      1.0          0         0   \n",
       "2         REG  TEN  ...          0            0      1.0          0         0   \n",
       "3         REG  MIA  ...          0            0      NaN          0         0   \n",
       "4         REG   LA  ...          0            0      NaN          0         0   \n",
       "\n",
       "   gwfg_missed  gwfg_blocked  gwfg_distance  fantasy_points  \\\n",
       "0            0             0              0            8.58   \n",
       "1            0             0              0            0.00   \n",
       "2            0             0              0            0.00   \n",
       "3            0             0              0            0.00   \n",
       "4            0             0              0           14.68   \n",
       "\n",
       "   fantasy_points_ppr  \n",
       "0                8.58  \n",
       "1                0.00  \n",
       "2                0.00  \n",
       "3                0.00  \n",
       "4               14.68  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and examine a sample of player data\n",
    "sample_year = \"2024\"\n",
    "player_sample = pd.read_csv(player_weekly_dir / f\"stats_player_week_{sample_year}.csv\")\n",
    "\n",
    "print(f\"Player data sample ({sample_year}):\")\n",
    "print(f\"Shape: {player_sample.shape}\")\n",
    "print(f\"Columns: {list(player_sample.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "player_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83a5aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team data sample (2024):\n",
      "Shape: (570, 102)\n",
      "Columns: ['season', 'week', 'team', 'season_type', 'opponent_team', 'completions', 'attempts', 'passing_yards', 'passing_tds', 'passing_interceptions', 'sacks_suffered', 'sack_yards_lost', 'sack_fumbles', 'sack_fumbles_lost', 'passing_air_yards', 'passing_yards_after_catch', 'passing_first_downs', 'passing_epa', 'passing_cpoe', 'passing_2pt_conversions', 'carries', 'rushing_yards', 'rushing_tds', 'rushing_fumbles', 'rushing_fumbles_lost', 'rushing_first_downs', 'rushing_epa', 'rushing_2pt_conversions', 'receptions', 'targets', 'receiving_yards', 'receiving_tds', 'receiving_fumbles', 'receiving_fumbles_lost', 'receiving_air_yards', 'receiving_yards_after_catch', 'receiving_first_downs', 'receiving_epa', 'receiving_2pt_conversions', 'special_teams_tds', 'def_tackles_solo', 'def_tackles_with_assist', 'def_tackle_assists', 'def_tackles_for_loss', 'def_tackles_for_loss_yards', 'def_fumbles_forced', 'def_sacks', 'def_sack_yards', 'def_qb_hits', 'def_interceptions', 'def_interception_yards', 'def_pass_defended', 'def_tds', 'def_fumbles', 'def_safeties', 'misc_yards', 'fumble_recovery_own', 'fumble_recovery_yards_own', 'fumble_recovery_opp', 'fumble_recovery_yards_opp', 'fumble_recovery_tds', 'penalties', 'penalty_yards', 'timeouts', 'punt_returns', 'punt_return_yards', 'kickoff_returns', 'kickoff_return_yards', 'fg_made', 'fg_att', 'fg_missed', 'fg_blocked', 'fg_long', 'fg_pct', 'fg_made_0_19', 'fg_made_20_29', 'fg_made_30_39', 'fg_made_40_49', 'fg_made_50_59', 'fg_made_60_', 'fg_missed_0_19', 'fg_missed_20_29', 'fg_missed_30_39', 'fg_missed_40_49', 'fg_missed_50_59', 'fg_missed_60_', 'fg_made_list', 'fg_missed_list', 'fg_blocked_list', 'fg_made_distance', 'fg_missed_distance', 'fg_blocked_distance', 'pat_made', 'pat_att', 'pat_missed', 'pat_blocked', 'pat_pct', 'gwfg_made', 'gwfg_att', 'gwfg_missed', 'gwfg_blocked', 'gwfg_distance']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>team</th>\n",
       "      <th>season_type</th>\n",
       "      <th>opponent_team</th>\n",
       "      <th>completions</th>\n",
       "      <th>attempts</th>\n",
       "      <th>passing_yards</th>\n",
       "      <th>passing_tds</th>\n",
       "      <th>passing_interceptions</th>\n",
       "      <th>...</th>\n",
       "      <th>pat_made</th>\n",
       "      <th>pat_att</th>\n",
       "      <th>pat_missed</th>\n",
       "      <th>pat_blocked</th>\n",
       "      <th>pat_pct</th>\n",
       "      <th>gwfg_made</th>\n",
       "      <th>gwfg_att</th>\n",
       "      <th>gwfg_missed</th>\n",
       "      <th>gwfg_blocked</th>\n",
       "      <th>gwfg_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>ARI</td>\n",
       "      <td>REG</td>\n",
       "      <td>BUF</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>ATL</td>\n",
       "      <td>REG</td>\n",
       "      <td>PIT</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>BAL</td>\n",
       "      <td>REG</td>\n",
       "      <td>KC</td>\n",
       "      <td>26</td>\n",
       "      <td>41</td>\n",
       "      <td>273</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>BUF</td>\n",
       "      <td>REG</td>\n",
       "      <td>ARI</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>232</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>CAR</td>\n",
       "      <td>REG</td>\n",
       "      <td>NO</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  week team season_type opponent_team  completions  attempts  \\\n",
       "0    2024     1  ARI         REG           BUF           21        31   \n",
       "1    2024     1  ATL         REG           PIT           16        26   \n",
       "2    2024     1  BAL         REG            KC           26        41   \n",
       "3    2024     1  BUF         REG           ARI           18        23   \n",
       "4    2024     1  CAR         REG            NO           13        31   \n",
       "\n",
       "   passing_yards  passing_tds  passing_interceptions  ...  pat_made  pat_att  \\\n",
       "0            162            1                      0  ...         2        2   \n",
       "1            155            1                      2  ...         1        1   \n",
       "2            273            1                      0  ...         2        2   \n",
       "3            232            2                      0  ...         4        4   \n",
       "4            161            0                      2  ...         1        1   \n",
       "\n",
       "   pat_missed  pat_blocked  pat_pct  gwfg_made  gwfg_att  gwfg_missed  \\\n",
       "0           0            0      1.0          0         0            0   \n",
       "1           0            0      1.0          0         0            0   \n",
       "2           0            0      1.0          0         0            0   \n",
       "3           0            0      1.0          0         0            0   \n",
       "4           0            0      1.0          0         0            0   \n",
       "\n",
       "   gwfg_blocked  gwfg_distance  \n",
       "0             0              0  \n",
       "1             0              0  \n",
       "2             0              0  \n",
       "3             0              0  \n",
       "4             0              0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and examine team data\n",
    "team_sample = pd.read_csv(team_weekly_dir / f\"stats_team_week_{sample_year}.csv\")\n",
    "\n",
    "print(f\"Team data sample ({sample_year}):\")\n",
    "print(f\"Shape: {team_sample.shape}\")\n",
    "print(f\"Columns: {list(team_sample.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "team_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb4cfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Players metadata:\n",
      "Shape: (24302, 39)\n",
      "Key columns: ['gsis_id', 'display_name', 'position', 'position_group']\n",
      "\n",
      "Position distribution:\n",
      "position_group\n",
      "DB      4357\n",
      "OL      4002\n",
      "DL      3421\n",
      "LB      3357\n",
      "WR      3169\n",
      "RB      2625\n",
      "TE      1545\n",
      "QB       990\n",
      "SPEC     836\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample players:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gsis_id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>position</th>\n",
       "      <th>position_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-0028830</td>\n",
       "      <td>Isaako Aaitui</td>\n",
       "      <td>NT</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-0038389</td>\n",
       "      <td>Israel Abanikanda</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-0024644</td>\n",
       "      <td>Jon Abbate</td>\n",
       "      <td>LB</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABB498348</td>\n",
       "      <td>Vince Abbott</td>\n",
       "      <td>K</td>\n",
       "      <td>SPEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-0031021</td>\n",
       "      <td>Jared Abbrederis</td>\n",
       "      <td>WR</td>\n",
       "      <td>WR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00-0032860</td>\n",
       "      <td>Mehdi Abdesmad</td>\n",
       "      <td>DE</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00-0028564</td>\n",
       "      <td>Isa Abdul-Quddus</td>\n",
       "      <td>S</td>\n",
       "      <td>DB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00-0032104</td>\n",
       "      <td>Ameer Abdullah</td>\n",
       "      <td>RB</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00-0023663</td>\n",
       "      <td>Hamza Abdullah</td>\n",
       "      <td>DB</td>\n",
       "      <td>DB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00-0025940</td>\n",
       "      <td>Husain Abdullah</td>\n",
       "      <td>FS</td>\n",
       "      <td>DB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gsis_id       display_name position position_group\n",
       "0  00-0028830      Isaako Aaitui       NT             DL\n",
       "1  00-0038389  Israel Abanikanda       RB             RB\n",
       "2  00-0024644         Jon Abbate       LB             LB\n",
       "3   ABB498348       Vince Abbott        K           SPEC\n",
       "4  00-0031021   Jared Abbrederis       WR             WR\n",
       "5  00-0032860     Mehdi Abdesmad       DE             DL\n",
       "6  00-0028564   Isa Abdul-Quddus        S             DB\n",
       "7  00-0032104     Ameer Abdullah       RB             RB\n",
       "8  00-0023663     Hamza Abdullah       DB             DB\n",
       "9  00-0025940    Husain Abdullah       FS             DB"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load players metadata\n",
    "players_meta = pd.read_csv(players_file)\n",
    "\n",
    "print(f\"Players metadata:\")\n",
    "print(f\"Shape: {players_meta.shape}\")\n",
    "print(f\"Key columns: {['gsis_id', 'display_name', 'position', 'position_group']}\")\n",
    "print(f\"\\nPosition distribution:\")\n",
    "print(players_meta['position_group'].value_counts())\n",
    "print(f\"\\nSample players:\")\n",
    "players_meta[['gsis_id', 'display_name', 'position', 'position_group']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1504a",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Pipeline\n",
    "\n",
    "Now let's build a comprehensive data preprocessing pipeline that:\n",
    "1. Loads all historical data\n",
    "2. Merges player and team information\n",
    "3. Creates temporal features\n",
    "4. Handles missing values and data quality issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81c172fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataPreprocessor initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Comprehensive data preprocessing pipeline for sports betting analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.player_weekly_dir = self.data_dir / \"player_weekly_stats\"\n",
    "        self.team_weekly_dir = self.data_dir / \"team_season_stats\"\n",
    "        self.players_file = self.data_dir / \"players.csv\"\n",
    "        \n",
    "        # Load players metadata once\n",
    "        self.players_meta = pd.read_csv(self.players_file)\n",
    "        \n",
    "        # Define key stats for each position\n",
    "        self.position_stats = {\n",
    "            'QB': ['passing_yards', 'passing_tds', 'passing_interceptions', 'completions', 'attempts'],\n",
    "            'RB': ['rushing_yards', 'rushing_tds', 'carries', 'receptions', 'receiving_yards'],\n",
    "            'WR': ['receiving_yards', 'receptions', 'receiving_tds', 'targets'],\n",
    "            'TE': ['receiving_yards', 'receptions', 'receiving_tds', 'targets'],\n",
    "            'K': ['fg_made', 'fg_att', 'pat_made', 'pat_att']\n",
    "        }\n",
    "        \n",
    "    def load_all_player_data(self, start_year=1999, end_year=2024):\n",
    "        \"\"\"Load all player weekly data for specified years.\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for year in range(start_year, end_year + 1):\n",
    "            file_path = self.player_weekly_dir / f\"stats_player_week_{year}.csv\"\n",
    "            if file_path.exists():\n",
    "                df = pd.read_csv(file_path)\n",
    "                all_data.append(df)\n",
    "                print(f\"Loaded {year}: {df.shape[0]} records\")\n",
    "            else:\n",
    "                print(f\"Warning: No data found for {year}\")\n",
    "        \n",
    "        if all_data:\n",
    "            combined = pd.concat(all_data, ignore_index=True)\n",
    "            print(f\"Total player records loaded: {combined.shape[0]}\")\n",
    "            return combined\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_all_team_data(self, start_year=1999, end_year=2024):\n",
    "        \"\"\"Load all team weekly data for specified years.\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for year in range(start_year, end_year + 1):\n",
    "            file_path = self.team_weekly_dir / f\"stats_team_week_{year}.csv\"\n",
    "            if file_path.exists():\n",
    "                df = pd.read_csv(file_path)\n",
    "                all_data.append(df)\n",
    "                print(f\"Loaded team data {year}: {df.shape[0]} records\")\n",
    "            else:\n",
    "                print(f\"Warning: No team data found for {year}\")\n",
    "        \n",
    "        if all_data:\n",
    "            combined = pd.concat(all_data, ignore_index=True)\n",
    "            print(f\"Total team records loaded: {combined.shape[0]}\")\n",
    "            return combined\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(\"../datasets\")\n",
    "print(\"DataPreprocessor initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b0be47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading recent data for testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2020: 17602 records\n",
      "Loaded 2021: 18969 records\n",
      "Loaded 2022: 18831 records\n",
      "Loaded 2023: 18643 records\n",
      "Loaded 2024: 18981 records\n",
      "Total player records loaded: 93026\n",
      "Loaded team data 2020: 538 records\n",
      "Loaded team data 2021: 570 records\n",
      "Loaded team data 2022: 568 records\n",
      "Loaded team data 2023: 570 records\n",
      "Loaded team data 2024: 570 records\n",
      "Total team records loaded: 2816\n",
      "\n",
      "Data loaded:\n",
      "Player data shape: (93026, 114)\n",
      "Team data shape: (2816, 102)\n",
      "\n",
      "Player data info:\n",
      "Date range: 2020 - 2024\n",
      "Unique players: 3676\n",
      "Positions: position\n",
      "WR     12670\n",
      "LB     10349\n",
      "CB     10121\n",
      "DE      8344\n",
      "RB      7883\n",
      "DT      7394\n",
      "TE      6272\n",
      "SAF     4595\n",
      "QB      3384\n",
      "K       2813\n",
      "P       2777\n",
      "DB      2664\n",
      "OLB     2239\n",
      "OT      2120\n",
      "FS      1704\n",
      "G       1545\n",
      "S       1294\n",
      "MLB     1080\n",
      "ILB     1063\n",
      "C        843\n",
      "NT       592\n",
      "FB       564\n",
      "LS       357\n",
      "DL       175\n",
      "OL        75\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Team data info:\n",
      "Date range: 2020 - 2024\n",
      "Unique teams: 32\n"
     ]
    }
   ],
   "source": [
    "# Load a subset of data for initial testing (2020-2024)\n",
    "print(\"Loading recent data for testing...\")\n",
    "player_data = preprocessor.load_all_player_data(start_year=2020, end_year=2024)\n",
    "team_data = preprocessor.load_all_team_data(start_year=2020, end_year=2024)\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"Player data shape: {player_data.shape}\")\n",
    "print(f\"Team data shape: {team_data.shape}\")\n",
    "\n",
    "# Basic data quality check\n",
    "print(f\"\\nPlayer data info:\")\n",
    "print(f\"Date range: {player_data['season'].min()} - {player_data['season'].max()}\")\n",
    "print(f\"Unique players: {player_data['player_id'].nunique()}\")\n",
    "print(f\"Positions: {player_data['position'].value_counts()}\")\n",
    "\n",
    "print(f\"\\nTeam data info:\")\n",
    "print(f\"Date range: {team_data['season'].min()} - {team_data['season'].max()}\")\n",
    "print(f\"Unique teams: {team_data['team'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe3ab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player data columns (first 20):\n",
      "['player_id', 'player_name', 'player_display_name', 'position', 'position_group', 'headshot_url', 'season', 'week', 'season_type', 'team', 'opponent_team', 'completions', 'attempts', 'passing_yards', 'passing_tds', 'passing_interceptions', 'sacks_suffered', 'sack_yards_lost', 'sack_fumbles', 'sack_fumbles_lost']\n",
      "\n",
      "Key statistical columns:\n",
      "['completions', 'attempts', 'passing_yards', 'passing_tds', 'sack_yards_lost', 'passing_air_yards', 'passing_yards_after_catch', 'carries', 'rushing_yards', 'rushing_tds', 'receptions', 'targets', 'receiving_yards', 'receiving_tds', 'receiving_air_yards']\n",
      "\n",
      "Sample QB data:\n",
      "        player_name  season  week team  passing_yards  passing_tds  \\\n",
      "0           T.Brady    2020     1   TB            239            2   \n",
      "1           D.Brees    2020     1   NO            160            2   \n",
      "5  B.Roethlisberger    2020     1  PIT            229            3   \n",
      "6          P.Rivers    2020     1  IND            363            1   \n",
      "8         A.Rodgers    2020     1   GB            364            4   \n",
      "\n",
      "   completions  attempts  \n",
      "0           23        36  \n",
      "1           18        30  \n",
      "5           21        32  \n",
      "6           36        46  \n",
      "8           32        44  \n"
     ]
    }
   ],
   "source": [
    "# Let's examine the data structure more closely\n",
    "print(\"Player data columns (first 20):\")\n",
    "print(player_data.columns[:20].tolist())\n",
    "\n",
    "print(\"\\nKey statistical columns:\")\n",
    "stat_cols = [col for col in player_data.columns if any(stat in col for stat in \n",
    "           ['yards', 'tds', 'completions', 'attempts', 'receptions', 'targets', 'carries'])]\n",
    "print(stat_cols[:15])\n",
    "\n",
    "print(\"\\nSample QB data:\")\n",
    "qb_data = player_data[player_data['position'] == 'QB'].head()\n",
    "print(qb_data[['player_name', 'season', 'week', 'team', 'passing_yards', 'passing_tds', 'completions', 'attempts']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca7fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering methods added to DataPreprocessor!\n"
     ]
    }
   ],
   "source": [
    "# Add methods to DataPreprocessor for feature engineering\n",
    "def add_temporal_features(self, df):\n",
    "    \"\"\"Add temporal features like season progression, home/away, etc.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create game identifier\n",
    "    df['game_id'] = df['season'].astype(str) + '_' + df['week'].astype(str) + '_' + df['team']\n",
    "    \n",
    "    # Season progression (week as fraction of season)\n",
    "    df['season_progression'] = df['week'] / 18.0  # Assuming 18-week season\n",
    "    \n",
    "    # Playoff indicator\n",
    "    df['is_playoff'] = (df['season_type'] == 'POST').astype(int)\n",
    "    \n",
    "    # Home/away (we'll need to infer this from opponent data)\n",
    "    # For now, we'll create a placeholder\n",
    "    df['is_home'] = 0  # Placeholder - would need schedule data\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_rolling_features(self, df, player_id_col='player_id', stat_cols=None, windows=[3, 5]):\n",
    "    \"\"\"Create rolling averages for key statistics.\"\"\"\n",
    "    if stat_cols is None:\n",
    "        stat_cols = ['passing_yards', 'rushing_yards', 'receiving_yards', 'receptions', 'targets']\n",
    "    \n",
    "    df = df.sort_values(['player_id', 'season', 'week'])\n",
    "    \n",
    "    for window in windows:\n",
    "        for stat in stat_cols:\n",
    "            if stat in df.columns:\n",
    "                # Rolling mean\n",
    "                df[f'{stat}_avg_{window}'] = df.groupby('player_id')[stat].rolling(\n",
    "                    window=window, min_periods=1\n",
    "                ).mean().reset_index(0, drop=True)\n",
    "                \n",
    "                # Rolling std\n",
    "                df[f'{stat}_std_{window}'] = df.groupby('player_id')[stat].rolling(\n",
    "                    window=window, min_periods=1\n",
    "                ).std().reset_index(0, drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_team_context(self, player_df, team_df):\n",
    "    \"\"\"Merge team-level context features.\"\"\"\n",
    "    # Create team game identifier\n",
    "    team_df = team_df.copy()\n",
    "    team_df['team_game_id'] = team_df['season'].astype(str) + '_' + team_df['week'].astype(str) + '_' + team_df['team']\n",
    "    \n",
    "    # Select relevant team features (only numeric ones)\n",
    "    team_features = ['passing_yards', 'rushing_yards', 'receptions', 'targets']\n",
    "    team_subset = team_df[['team_game_id'] + team_features].copy()\n",
    "    \n",
    "    # Rename columns to indicate team context\n",
    "    team_subset.columns = ['team_game_id'] + [f'team_{col}' for col in team_features]\n",
    "    \n",
    "    # Merge with player data\n",
    "    player_df = player_df.merge(team_subset, left_on='game_id', right_on='team_game_id', how='left')\n",
    "    \n",
    "    # Drop the team_game_id column to avoid data type issues\n",
    "    player_df = player_df.drop('team_game_id', axis=1)\n",
    "    \n",
    "    return player_df\n",
    "\n",
    "# Add methods to the class\n",
    "DataPreprocessor.add_temporal_features = add_temporal_features\n",
    "DataPreprocessor.create_rolling_features = create_rolling_features\n",
    "DataPreprocessor.merge_team_context = merge_team_context\n",
    "\n",
    "print(\"Feature engineering methods added to DataPreprocessor!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17339c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing pipeline...\n",
      "Sample data shapes: Players (1000, 114), Teams (500, 102)\n",
      "After temporal features: (1000, 118)\n",
      "After rolling features: (1000, 126)\n",
      "After team context: (1000, 130)\n",
      "\n",
      "Sample of processed data:\n",
      "  player_name  season  week  passing_yards  passing_yards_avg_3  \\\n",
      "0     T.Brady    2020     1            239           239.000000   \n",
      "1     T.Brady    2020     2            217           228.000000   \n",
      "2     T.Brady    2020     3            297           251.000000   \n",
      "3     T.Brady    2020     4            369           294.333333   \n",
      "4     T.Brady    2020     5            253           306.333333   \n",
      "\n",
      "   team_passing_yards  \n",
      "0               239.0  \n",
      "1               217.0  \n",
      "2               297.0  \n",
      "3               369.0  \n",
      "4               253.0  \n"
     ]
    }
   ],
   "source": [
    "# Test the preprocessing pipeline on a small sample\n",
    "print(\"Testing preprocessing pipeline...\")\n",
    "\n",
    "# Take a small sample for testing\n",
    "sample_players = player_data[player_data['position'] == 'QB'].head(1000)\n",
    "sample_teams = team_data.head(500)\n",
    "\n",
    "print(f\"Sample data shapes: Players {sample_players.shape}, Teams {sample_teams.shape}\")\n",
    "\n",
    "# Add temporal features\n",
    "sample_players = preprocessor.add_temporal_features(sample_players)\n",
    "print(f\"After temporal features: {sample_players.shape}\")\n",
    "\n",
    "# Create rolling features for QBs\n",
    "qb_stats = ['passing_yards', 'passing_tds', 'completions', 'attempts']\n",
    "sample_players = preprocessor.create_rolling_features(sample_players, stat_cols=qb_stats, windows=[3])\n",
    "print(f\"After rolling features: {sample_players.shape}\")\n",
    "\n",
    "# Merge team context\n",
    "sample_players = preprocessor.merge_team_context(sample_players, sample_teams)\n",
    "print(f\"After team context: {sample_players.shape}\")\n",
    "\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(sample_players[['player_name', 'season', 'week', 'passing_yards', 'passing_yards_avg_3', 'team_passing_yards']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957f3aa",
   "metadata": {},
   "source": [
    "## 3. Model Training Pipeline\n",
    "\n",
    "Now let's implement the core ML pipeline with per-position LightGBM quantile models for predicting player prop outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81cd4480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelTrainer initialized!\n"
     ]
    }
   ],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"LightGBM quantile models for per-position player prop predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.feature_columns = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def prepare_features(self, df, position, target_stat):\n",
    "        \"\"\"Prepare features for a specific position and target statistic.\"\"\"\n",
    "        # Get position-specific stats\n",
    "        if position == 'QB':\n",
    "            feature_stats = ['passing_yards', 'passing_tds', 'completions', 'attempts', 'passing_interceptions']\n",
    "        elif position == 'RB':\n",
    "            feature_stats = ['rushing_yards', 'rushing_tds', 'carries', 'receptions', 'receiving_yards']\n",
    "        elif position in ['WR', 'TE']:\n",
    "            feature_stats = ['receiving_yards', 'receptions', 'receiving_tds', 'targets']\n",
    "        else:\n",
    "            feature_stats = []\n",
    "        \n",
    "        # Base features\n",
    "        base_features = ['season_progression', 'is_playoff', 'is_home']\n",
    "        \n",
    "        # Rolling features\n",
    "        rolling_features = []\n",
    "        for stat in feature_stats:\n",
    "            for window in [3, 5]:\n",
    "                rolling_features.extend([f'{stat}_avg_{window}', f'{stat}_std_{window}'])\n",
    "        \n",
    "        # Team context features (only numeric ones)\n",
    "        team_features = [col for col in df.columns if col.startswith('team_') and col != 'team_game_id']\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = base_features + rolling_features + team_features\n",
    "        \n",
    "        # Filter to available features and ensure they're numeric\n",
    "        available_features = []\n",
    "        for f in all_features:\n",
    "            if f in df.columns:\n",
    "                # Check if the column is numeric\n",
    "                if pd.api.types.is_numeric_dtype(df[f]):\n",
    "                    available_features.append(f)\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping non-numeric feature: {f}\")\n",
    "        \n",
    "        return available_features\n",
    "    \n",
    "    def train_position_model(self, df, position, target_stat, quantiles=[0.1, 0.5, 0.9]):\n",
    "        \"\"\"Train quantile models for a specific position and target statistic.\"\"\"\n",
    "        print(f\"Training {position} model for {target_stat}...\")\n",
    "        \n",
    "        # Filter data for position\n",
    "        pos_data = df[df['position'] == position].copy()\n",
    "        \n",
    "        if len(pos_data) < 100:\n",
    "            print(f\"Warning: Not enough data for {position} ({len(pos_data)} records)\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features\n",
    "        feature_cols = self.prepare_features(pos_data, position, target_stat)\n",
    "        \n",
    "        if len(feature_cols) == 0:\n",
    "            print(f\"Warning: No features available for {position}\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare data\n",
    "        X = pos_data[feature_cols].fillna(0)\n",
    "        y = pos_data[target_stat].fillna(0)\n",
    "        \n",
    "        # Remove rows where target is missing\n",
    "        valid_mask = ~y.isna()\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        \n",
    "        if len(X) < 50:\n",
    "            print(f\"Warning: Not enough valid data for {position} ({len(X)} records)\")\n",
    "            return None\n",
    "        \n",
    "        # Store feature columns\n",
    "        self.feature_columns[f\"{position}_{target_stat}\"] = feature_cols\n",
    "        \n",
    "        # Train models for different quantiles\n",
    "        position_models = {}\n",
    "        \n",
    "        for alpha in quantiles:\n",
    "            print(f\"  Training quantile {alpha}...\")\n",
    "            \n",
    "            # LightGBM parameters\n",
    "            params = {\n",
    "                'objective': 'quantile',\n",
    "                'alpha': alpha,\n",
    "                'learning_rate': 0.05,\n",
    "                'num_leaves': 31,\n",
    "                'min_data_in_leaf': 20,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'verbose': -1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "            \n",
    "            # Create and train model\n",
    "            train_data = lgb.Dataset(X, label=y)\n",
    "            model = lgb.train(params, train_data, num_boost_round=100)\n",
    "            \n",
    "            position_models[alpha] = model\n",
    "        \n",
    "        self.models[f\"{position}_{target_stat}\"] = position_models\n",
    "        print(f\"  {position} {target_stat} model trained successfully!\")\n",
    "        \n",
    "        return position_models\n",
    "\n",
    "# Initialize model trainer\n",
    "model_trainer = ModelTrainer()\n",
    "print(\"ModelTrainer initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0305f349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running complete preprocessing pipeline...\n",
      "Starting complete preprocessing pipeline...\n",
      "Filtered data: Players (93026, 114), Teams (2816, 102)\n",
      "Adding temporal features...\n",
      "Creating rolling features...\n",
      "  Processing QB (3384 records)...\n",
      "  Processing RB (7883 records)...\n",
      "  Processing WR (12670 records)...\n",
      "  Processing TE (6272 records)...\n",
      "Merging team context...\n",
      "Preprocessing complete! Final shape: (93026, 166)\n",
      "\n",
      "Processed data summary:\n",
      "Shape: (93026, 166)\n",
      "Columns: 166\n",
      "Positions: position\n",
      "WR     12670\n",
      "LB     10349\n",
      "CB     10121\n",
      "DE      8344\n",
      "RB      7883\n",
      "DT      7394\n",
      "TE      6272\n",
      "SAF     4595\n",
      "QB      3384\n",
      "K       2813\n",
      "P       2777\n",
      "DB      2664\n",
      "OLB     2239\n",
      "OT      2120\n",
      "FS      1704\n",
      "G       1545\n",
      "S       1294\n",
      "MLB     1080\n",
      "ILB     1063\n",
      "C        843\n",
      "NT       592\n",
      "FB       564\n",
      "LS       357\n",
      "DL       175\n",
      "OL        75\n",
      "Name: count, dtype: int64\n",
      "Years: [2020 2021 2022 2023 2024]\n",
      "\n",
      "Sample engineered features: ['season_progression', 'passing_yards_avg_3', 'passing_yards_std_3', 'passing_tds_avg_3', 'passing_tds_std_3', 'completions_avg_3', 'completions_std_3', 'attempts_avg_3', 'attempts_std_3', 'passing_yards_avg_5']\n"
     ]
    }
   ],
   "source": [
    "# Complete preprocessing pipeline function\n",
    "def complete_preprocessing_pipeline(player_data, team_data, start_year=2020, end_year=2024):\n",
    "    \"\"\"Complete preprocessing pipeline for all data.\"\"\"\n",
    "    print(\"Starting complete preprocessing pipeline...\")\n",
    "    \n",
    "    # Filter to recent years for testing\n",
    "    player_data = player_data[(player_data['season'] >= start_year) & (player_data['season'] <= end_year)]\n",
    "    team_data = team_data[(team_data['season'] >= start_year) & (team_data['season'] <= end_year)]\n",
    "    \n",
    "    print(f\"Filtered data: Players {player_data.shape}, Teams {team_data.shape}\")\n",
    "    \n",
    "    # Add temporal features\n",
    "    print(\"Adding temporal features...\")\n",
    "    player_data = preprocessor.add_temporal_features(player_data)\n",
    "    \n",
    "    # Create rolling features for each position\n",
    "    print(\"Creating rolling features...\")\n",
    "    positions = ['QB', 'RB', 'WR', 'TE']\n",
    "    \n",
    "    for position in positions:\n",
    "        pos_data = player_data[player_data['position'] == position]\n",
    "        if len(pos_data) > 0:\n",
    "            print(f\"  Processing {position} ({len(pos_data)} records)...\")\n",
    "            \n",
    "            # Get position-specific stats\n",
    "            if position == 'QB':\n",
    "                stats = ['passing_yards', 'passing_tds', 'completions', 'attempts']\n",
    "            elif position == 'RB':\n",
    "                stats = ['rushing_yards', 'rushing_tds', 'carries', 'receptions', 'receiving_yards']\n",
    "            elif position in ['WR', 'TE']:\n",
    "                stats = ['receiving_yards', 'receptions', 'receiving_tds', 'targets']\n",
    "            \n",
    "            # Create rolling features\n",
    "            pos_data = preprocessor.create_rolling_features(pos_data, stat_cols=stats, windows=[3, 5])\n",
    "            \n",
    "            # Update the main dataframe\n",
    "            player_data.loc[player_data['position'] == position, pos_data.columns] = pos_data\n",
    "    \n",
    "    # Merge team context\n",
    "    print(\"Merging team context...\")\n",
    "    player_data = preprocessor.merge_team_context(player_data, team_data)\n",
    "    \n",
    "    print(f\"Preprocessing complete! Final shape: {player_data.shape}\")\n",
    "    return player_data\n",
    "\n",
    "# Run the complete preprocessing pipeline\n",
    "print(\"Running complete preprocessing pipeline...\")\n",
    "processed_data = complete_preprocessing_pipeline(player_data, team_data)\n",
    "\n",
    "print(f\"\\nProcessed data summary:\")\n",
    "print(f\"Shape: {processed_data.shape}\")\n",
    "print(f\"Columns: {len(processed_data.columns)}\")\n",
    "print(f\"Positions: {processed_data['position'].value_counts()}\")\n",
    "print(f\"Years: {processed_data['season'].unique()}\")\n",
    "\n",
    "# Show sample of processed features\n",
    "feature_cols = [col for col in processed_data.columns if any(x in col for x in ['_avg_', '_std_', 'team_', 'season_progression'])]\n",
    "print(f\"\\nSample engineered features: {feature_cols[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b35b21",
   "metadata": {},
   "source": [
    "# Testing on QB players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed304d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model training with fixed preprocessing...\n",
      "QB data shape: (3384, 166)\n",
      "Training QB model for passing_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  QB passing_yards model trained successfully!\n",
      "✅ QB model training successful!\n",
      "Available quantiles: [0.1, 0.5, 0.9]\n",
      "Features used: ['season_progression', 'is_playoff', 'is_home', 'passing_yards_avg_3', 'passing_yards_std_3', 'passing_yards_avg_5', 'passing_yards_std_5', 'passing_tds_avg_3', 'passing_tds_std_3', 'passing_tds_avg_5', 'passing_tds_std_5', 'completions_avg_3', 'completions_std_3', 'completions_avg_5', 'completions_std_5', 'attempts_avg_3', 'attempts_std_3', 'attempts_avg_5', 'attempts_std_5', 'team_passing_yards', 'team_rushing_yards', 'team_receptions', 'team_targets']\n",
      "\n",
      "Sample predictions for first 5 QBs:\n",
      "Quantile 0.1: [223.4110042  155.07472993 213.71078534 279.46583928 279.46583928]\n",
      "Quantile 0.5: [239.33534605 191.32535316 229.20478219 359.31934774 364.5018578 ]\n",
      "Quantile 0.9: [240.55288213 201.89329392 231.52190908 364.52073535 366.45491588]\n",
      "\n",
      "Actual vs Predicted (median):\n",
      "Player 1: Actual=239.0, Predicted=239.3\n",
      "Player 2: Actual=160.0, Predicted=191.3\n",
      "Player 3: Actual=229.0, Predicted=229.2\n",
      "Player 4: Actual=363.0, Predicted=359.3\n",
      "Player 5: Actual=364.0, Predicted=364.5\n"
     ]
    }
   ],
   "source": [
    "# Test model training again with fixed preprocessing\n",
    "print(\"Testing model training with fixed preprocessing...\")\n",
    "\n",
    "# Get QB data\n",
    "qb_data = processed_data[processed_data['position'] == 'QB'].copy()\n",
    "print(f\"QB data shape: {qb_data.shape}\")\n",
    "\n",
    "if len(qb_data) > 100:\n",
    "    # Train a QB passing yards model\n",
    "    qb_model = model_trainer.train_position_model(qb_data, 'QB', 'passing_yards')\n",
    "    \n",
    "    if qb_model:\n",
    "        print(\"✅ QB model training successful!\")\n",
    "        print(f\"Available quantiles: {list(qb_model.keys())}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        feature_cols = model_trainer.feature_columns['QB_passing_yards']\n",
    "        print(f\"Features used: {feature_cols}\")\n",
    "        \n",
    "        sample_features = qb_data[feature_cols].fillna(0).iloc[:5]\n",
    "        \n",
    "        print(f\"\\nSample predictions for first 5 QBs:\")\n",
    "        for alpha, model in qb_model.items():\n",
    "            predictions = model.predict(sample_features)\n",
    "            print(f\"Quantile {alpha}: {predictions}\")\n",
    "            \n",
    "        # Show actual vs predicted for comparison\n",
    "        actual_values = qb_data['passing_yards'].iloc[:5].values\n",
    "        median_predictions = qb_model[0.5].predict(sample_features)\n",
    "        print(f\"\\nActual vs Predicted (median):\")\n",
    "        for i in range(5):\n",
    "            print(f\"Player {i+1}: Actual={actual_values[i]:.1f}, Predicted={median_predictions[i]:.1f}\")\n",
    "    else:\n",
    "        print(\"❌ QB model training failed!\")\n",
    "else:\n",
    "    print(\"Not enough QB data for training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f709d",
   "metadata": {},
   "source": [
    "# Testing on RB players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d42a7932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model training for RB rushing yards...\n",
      "RB data shape: (7883, 166)\n",
      "Training RB model for rushing_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  RB rushing_yards model trained successfully!\n",
      "✅ RB model training successful!\n",
      "Available quantiles: [0.1, 0.5, 0.9]\n",
      "Features used: ['season_progression', 'is_playoff', 'is_home', 'rushing_yards_avg_3', 'rushing_yards_std_3', 'rushing_yards_avg_5', 'rushing_yards_std_5', 'rushing_tds_avg_3', 'rushing_tds_std_3', 'rushing_tds_avg_5', 'rushing_tds_std_5', 'carries_avg_3', 'carries_std_3', 'carries_avg_5', 'carries_std_5', 'receptions_avg_3', 'receptions_std_3', 'receptions_avg_5', 'receptions_std_5', 'receiving_yards_avg_3', 'receiving_yards_std_3', 'receiving_yards_avg_5', 'receiving_yards_std_5', 'team_passing_yards', 'team_rushing_yards', 'team_receptions', 'team_targets']\n",
      "\n",
      "Sample predictions for first 5 RBs:\n",
      "Quantile 0.1: [15.48949427 51.44905816  0.         22.27390728  0.80480833]\n",
      "Quantile 0.5: [23.94372016 88.62110975  0.13029566 29.54709698  0.92069767]\n",
      "Quantile 0.9: [25.28104549 92.3430254  11.32011653 35.45381147 11.32011653]\n",
      "\n",
      "Actual vs Predicted (median):\n",
      "Player 1: Actual=24.0, Predicted=23.9\n",
      "Player 2: Actual=93.0, Predicted=88.6\n",
      "Player 3: Actual=0.0, Predicted=0.1\n",
      "Player 4: Actual=29.0, Predicted=29.5\n",
      "Player 5: Actual=1.0, Predicted=0.9\n",
      "\n",
      "RB names for context:\n",
      "F.Gore (2020 W1 NYJ): 24 yards\n",
      "A.Peterson (2020 W1 DET): 93 yards\n",
      "L.McCoy (2020 W1 TB): 0 yards\n",
      "M.Ingram (2020 W1 BAL): 29 yards\n",
      "D.Lewis (2020 W1 NYG): 1 yards\n"
     ]
    }
   ],
   "source": [
    "# Test model training for RB players\n",
    "print(\"Testing model training for RB rushing yards...\")\n",
    "\n",
    "# Get RB data\n",
    "rb_data = processed_data[processed_data['position'] == 'RB'].copy()\n",
    "print(f\"RB data shape: {rb_data.shape}\")\n",
    "\n",
    "if len(rb_data) > 100:\n",
    "    # Train a RB rushing yards model\n",
    "    rb_model = model_trainer.train_position_model(rb_data, 'RB', 'rushing_yards')\n",
    "    \n",
    "    if rb_model:\n",
    "        print(\"✅ RB model training successful!\")\n",
    "        print(f\"Available quantiles: {list(rb_model.keys())}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        feature_cols = model_trainer.feature_columns['RB_rushing_yards']\n",
    "        print(f\"Features used: {feature_cols}\")\n",
    "        \n",
    "        sample_features = rb_data[feature_cols].fillna(0).iloc[:5]\n",
    "        \n",
    "        print(f\"\\nSample predictions for first 5 RBs:\")\n",
    "        for alpha, model in rb_model.items():\n",
    "            predictions = model.predict(sample_features)\n",
    "            print(f\"Quantile {alpha}: {predictions}\")\n",
    "            \n",
    "        # Show actual vs predicted for comparison\n",
    "        actual_values = rb_data['rushing_yards'].iloc[:5].values\n",
    "        median_predictions = rb_model[0.5].predict(sample_features)\n",
    "        print(f\"\\nActual vs Predicted (median):\")\n",
    "        for i in range(5):\n",
    "            print(f\"Player {i+1}: Actual={actual_values[i]:.1f}, Predicted={median_predictions[i]:.1f}\")\n",
    "            \n",
    "        # Show some RB names for context\n",
    "        print(f\"\\nRB names for context:\")\n",
    "        rb_names = rb_data[['player_name', 'season', 'week', 'team', 'rushing_yards']].iloc[:5]\n",
    "        for i, row in rb_names.iterrows():\n",
    "            print(f\"{row['player_name']} ({row['season']} W{row['week']} {row['team']}): {row['rushing_yards']} yards\")\n",
    "    else:\n",
    "        print(\"❌ RB model training failed!\")\n",
    "else:\n",
    "    print(\"Not enough RB data for training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bbcf1",
   "metadata": {},
   "source": [
    "# Testing on WR players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76ec3b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model training for WR receiving yards...\n",
      "WR data shape: (12670, 166)\n",
      "Training WR model for receiving_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  WR receiving_yards model trained successfully!\n",
      "✅ WR model training successful!\n",
      "Available quantiles: [0.1, 0.5, 0.9]\n",
      "Features used: ['season_progression', 'is_playoff', 'is_home', 'receiving_yards_avg_3', 'receiving_yards_std_3', 'receiving_yards_avg_5', 'receiving_yards_std_5', 'receptions_avg_3', 'receptions_std_3', 'receptions_avg_5', 'receptions_std_5', 'receiving_tds_avg_3', 'receiving_tds_std_3', 'receiving_tds_avg_5', 'receiving_tds_std_5', 'targets_avg_3', 'targets_std_3', 'targets_avg_5', 'targets_std_5', 'team_passing_yards', 'team_rushing_yards', 'team_receptions', 'team_targets']\n",
      "\n",
      "Sample predictions for first 5 WRs:\n",
      "Quantile 0.1: [26.2764613   0.         47.11776055 27.88461741 40.57257287]\n",
      "Quantile 0.5: [33.85800957  0.13617217 81.3516545  45.70438828 57.47918917]\n",
      "Quantile 0.9: [37.87311891 11.89082583 80.74862874 48.14623757 60.65871372]\n",
      "\n",
      "Actual vs Predicted (median):\n",
      "Player 1: Actual=34.0, Predicted=33.9\n",
      "Player 2: Actual=0.0, Predicted=0.1\n",
      "Player 3: Actual=81.0, Predicted=81.4\n",
      "Player 4: Actual=46.0, Predicted=45.7\n",
      "Player 5: Actual=57.0, Predicted=57.5\n",
      "\n",
      "WR names for context:\n",
      "L.Fitzgerald (2020 W1 ARI): 34 yards\n",
      "T.Ginn (2020 W1 CHI): 0 yards\n",
      "D.Amendola (2020 W1 DET): 81 yards\n",
      "D.Jackson (2020 W1 PHI): 46 yards\n",
      "J.Edelman (2020 W1 NE): 57 yards\n"
     ]
    }
   ],
   "source": [
    "# Test model training for WR players\n",
    "print(\"Testing model training for WR receiving yards...\")\n",
    "\n",
    "# Get WR data\n",
    "wr_data = processed_data[processed_data['position'] == 'WR'].copy()\n",
    "print(f\"WR data shape: {wr_data.shape}\")\n",
    "\n",
    "if len(wr_data) > 100:\n",
    "    # Train a WR receiving yards model\n",
    "    wr_model = model_trainer.train_position_model(wr_data, 'WR', 'receiving_yards')\n",
    "    \n",
    "    if wr_model:\n",
    "        print(\"✅ WR model training successful!\")\n",
    "        print(f\"Available quantiles: {list(wr_model.keys())}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        feature_cols = model_trainer.feature_columns['WR_receiving_yards']\n",
    "        print(f\"Features used: {feature_cols}\")\n",
    "        \n",
    "        sample_features = wr_data[feature_cols].fillna(0).iloc[:5]\n",
    "        \n",
    "        print(f\"\\nSample predictions for first 5 WRs:\")\n",
    "        for alpha, model in wr_model.items():\n",
    "            predictions = model.predict(sample_features)\n",
    "            print(f\"Quantile {alpha}: {predictions}\")\n",
    "            \n",
    "        # Show actual vs predicted for comparison\n",
    "        actual_values = wr_data['receiving_yards'].iloc[:5].values\n",
    "        median_predictions = wr_model[0.5].predict(sample_features)\n",
    "        print(f\"\\nActual vs Predicted (median):\")\n",
    "        for i in range(5):\n",
    "            print(f\"Player {i+1}: Actual={actual_values[i]:.1f}, Predicted={median_predictions[i]:.1f}\")\n",
    "            \n",
    "        # Show some WR names for context\n",
    "        print(f\"\\nWR names for context:\")\n",
    "        wr_names = wr_data[['player_name', 'season', 'week', 'team', 'receiving_yards']].iloc[:5]\n",
    "        for i, row in wr_names.iterrows():\n",
    "            print(f\"{row['player_name']} ({row['season']} W{row['week']} {row['team']}): {row['receiving_yards']} yards\")\n",
    "    else:\n",
    "        print(\"❌ WR model training failed!\")\n",
    "else:\n",
    "    print(\"Not enough WR data for training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc44844",
   "metadata": {},
   "source": [
    "# Testing on TE players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e079cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model training for TE receiving yards...\n",
      "TE data shape: (6272, 166)\n",
      "Training TE model for receiving_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  TE receiving_yards model trained successfully!\n",
      "✅ TE model training successful!\n",
      "Available quantiles: [0.1, 0.5, 0.9]\n",
      "Features used: ['season_progression', 'is_playoff', 'is_home', 'receiving_yards_avg_3', 'receiving_yards_std_3', 'receiving_yards_avg_5', 'receiving_yards_std_5', 'receptions_avg_3', 'receptions_std_3', 'receptions_avg_5', 'receptions_std_5', 'receiving_tds_avg_3', 'receiving_tds_std_3', 'receiving_tds_avg_5', 'receiving_tds_std_5', 'targets_avg_3', 'targets_std_3', 'targets_avg_5', 'targets_std_5', 'team_passing_yards', 'team_rushing_yards', 'team_receptions', 'team_targets']\n",
      "\n",
      "Sample predictions for first 5 TEs:\n",
      "Quantile 0.1: [ 0.54666473 17.41584431 30.93322363  7.74989816 16.57240838]\n",
      "Quantile 0.5: [ 1.88864723 25.48091169 62.709778   11.07553146 24.97856811]\n",
      "Quantile 0.9: [ 9.43195133 28.36232317 80.01763438 14.64212154 27.62733042]\n",
      "\n",
      "Actual vs Predicted (median):\n",
      "Player 1: Actual=2.0, Predicted=1.9\n",
      "Player 2: Actual=24.0, Predicted=25.5\n",
      "Player 3: Actual=80.0, Predicted=62.7\n",
      "Player 4: Actual=11.0, Predicted=11.1\n",
      "Player 5: Actual=25.0, Predicted=25.0\n",
      "\n",
      "TE names for context:\n",
      "J.Witten (2020 W1 LV): 2 yards\n",
      "G.Olsen (2020 W1 SEA): 24 yards\n",
      "J.Cook (2020 W1 NO): 80 yards\n",
      "R.Gronkowski (2020 W1 TB): 11 yards\n",
      "J.Graham (2020 W1 CHI): 25 yards\n"
     ]
    }
   ],
   "source": [
    "# Test model training for TE players\n",
    "print(\"Testing model training for TE receiving yards...\")\n",
    "\n",
    "# Get TE data\n",
    "te_data = processed_data[processed_data['position'] == 'TE'].copy()\n",
    "print(f\"TE data shape: {te_data.shape}\")\n",
    "\n",
    "if len(te_data) > 100:\n",
    "    # Train a TE receiving yards model\n",
    "    te_model = model_trainer.train_position_model(te_data, 'TE', 'receiving_yards')\n",
    "    \n",
    "    if te_model:\n",
    "        print(\"✅ TE model training successful!\")\n",
    "        print(f\"Available quantiles: {list(te_model.keys())}\")\n",
    "        \n",
    "        # Test prediction\n",
    "        feature_cols = model_trainer.feature_columns['TE_receiving_yards']\n",
    "        print(f\"Features used: {feature_cols}\")\n",
    "        \n",
    "        sample_features = te_data[feature_cols].fillna(0).iloc[:5]\n",
    "        \n",
    "        print(f\"\\nSample predictions for first 5 TEs:\")\n",
    "        for alpha, model in te_model.items():\n",
    "            predictions = model.predict(sample_features)\n",
    "            print(f\"Quantile {alpha}: {predictions}\")\n",
    "            \n",
    "        # Show actual vs predicted for comparison\n",
    "        actual_values = te_data['receiving_yards'].iloc[:5].values\n",
    "        median_predictions = te_model[0.5].predict(sample_features)\n",
    "        print(f\"\\nActual vs Predicted (median):\")\n",
    "        for i in range(5):\n",
    "            print(f\"Player {i+1}: Actual={actual_values[i]:.1f}, Predicted={median_predictions[i]:.1f}\")\n",
    "            \n",
    "        # Show some TE names for context\n",
    "        print(f\"\\nTE names for context:\")\n",
    "        te_names = te_data[['player_name', 'season', 'week', 'team', 'receiving_yards']].iloc[:5]\n",
    "        for i, row in te_names.iterrows():\n",
    "            print(f\"{row['player_name']} ({row['season']} W{row['week']} {row['team']}): {row['receiving_yards']} yards\")\n",
    "    else:\n",
    "        print(\"❌ TE model training failed!\")\n",
    "else:\n",
    "    print(\"Not enough TE data for training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c609366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHECKING DATA AVAILABILITY FOR ALL BETTING STATS\n",
      "================================================================================\n",
      "\n",
      "📊 QB Position (3,384 total records):\n",
      "--------------------------------------------------------------------------------\n",
      "  ✅ READY         passing_yards             | Non-zero: 3,161 ( 93.4%) | Avg: 213.5\n",
      "  ✅ READY         passing_tds               | Non-zero: 2,273 ( 67.2%) | Avg: 1.8\n",
      "  ✅ READY         completions               | Non-zero: 3,168 ( 93.6%) | Avg: 19.4\n",
      "  ✅ READY         passing_interceptions     | Non-zero: 1,501 ( 44.4%) | Avg: 1.4\n",
      "  ✅ READY         rushing_yards             | Non-zero: 2,435 ( 72.0%) | Avg: 21.6\n",
      "\n",
      "📊 RB Position (7,883 total records):\n",
      "--------------------------------------------------------------------------------\n",
      "  ✅ READY         rushing_yards             | Non-zero: 6,459 ( 81.9%) | Avg: 40.7\n",
      "  ✅ READY         rushing_tds               | Non-zero: 1,575 ( 20.0%) | Avg: 1.2\n",
      "  ✅ READY         receptions                | Non-zero: 5,082 ( 64.5%) | Avg: 2.5\n",
      "  ✅ READY         receiving_yards           | Non-zero: 4,810 ( 61.0%) | Avg: 19.9\n",
      "  ✅ READY         receiving_tds             | Non-zero:   451 (  5.7%) | Avg: 1.1\n",
      "\n",
      "📊 WR Position (12,670 total records):\n",
      "--------------------------------------------------------------------------------\n",
      "  ✅ READY         receiving_yards           | Non-zero: 9,796 ( 77.3%) | Avg: 44.5\n",
      "  ✅ READY         receptions                | Non-zero: 9,916 ( 78.3%) | Avg: 3.5\n",
      "  ✅ READY         receiving_tds             | Non-zero: 2,319 ( 18.3%) | Avg: 1.1\n",
      "  ✅ READY         targets                   | Non-zero: 11,027 ( 87.0%) | Avg: 4.9\n",
      "\n",
      "📊 TE Position (6,272 total records):\n",
      "--------------------------------------------------------------------------------\n",
      "  ✅ READY         receiving_yards           | Non-zero: 4,900 ( 78.1%) | Avg: 29.4\n",
      "  ✅ READY         receptions                | Non-zero: 4,960 ( 79.1%) | Avg: 2.8\n",
      "  ✅ READY         receiving_tds             | Non-zero:   936 ( 14.9%) | Avg: 1.1\n",
      "\n",
      "================================================================================\n",
      "✅ Total trainable position-stat combinations: 17\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Check data availability for all stats before training\n",
    "# ============================================================================\n",
    "\n",
    "position_stats_check = {\n",
    "    'QB': ['passing_yards', 'passing_tds', 'completions', 'passing_interceptions', 'rushing_yards'],\n",
    "    'RB': ['rushing_yards', 'rushing_tds', 'receptions', 'receiving_yards', 'receiving_tds'],\n",
    "    'WR': ['receiving_yards', 'receptions', 'receiving_tds', 'targets'],\n",
    "    'TE': ['receiving_yards', 'receptions', 'receiving_tds']\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CHECKING DATA AVAILABILITY FOR ALL BETTING STATS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "available_configs = []\n",
    "\n",
    "for position, stats in position_stats_check.items():\n",
    "    pos_data = processed_data[processed_data['position'] == position]\n",
    "    print(f\"📊 {position} Position ({len(pos_data):,} total records):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for stat in stats:\n",
    "        if stat in pos_data.columns:\n",
    "            # Check how many non-null and non-zero values\n",
    "            non_null = pos_data[stat].notna().sum()\n",
    "            non_zero = (pos_data[stat] > 0).sum()\n",
    "            pct_non_zero = (non_zero / len(pos_data) * 100) if len(pos_data) > 0 else 0\n",
    "            \n",
    "            # Calculate some basic stats\n",
    "            mean_val = pos_data[pos_data[stat] > 0][stat].mean()\n",
    "            \n",
    "            status = \"✅ READY\" if non_zero > 100 else \"⚠️ LIMITED DATA\"\n",
    "            print(f\"  {status:15} {stat:25} | Non-zero: {non_zero:5,} ({pct_non_zero:5.1f}%) | Avg: {mean_val:.1f}\")\n",
    "            \n",
    "            if non_zero > 100:\n",
    "                available_configs.append((position, stat))\n",
    "        else:\n",
    "            print(f\"  ❌ NOT FOUND   {stat:25} | Column does not exist in data\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"✅ Total trainable position-stat combinations: {len(available_configs)}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67e28b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE MODEL SUITE\n",
      "================================================================================\n",
      "\n",
      "📋 Configured to train 17 models\n",
      "🔄 Training with quantiles: [0.1, 0.5, 0.9]\n",
      "💾 Each model will be saved for production use\n",
      "\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 1/17: QB - passing_yards\n",
      "================================================================================\n",
      "📊 Training on 3,384 records (3,384 valid)\n",
      "Training QB model for passing_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  QB passing_yards model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=199.4, Median=214.0, StdDev=105.6\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 2/17: QB - passing_tds\n",
      "================================================================================\n",
      "📊 Training on 3,384 records (3,384 valid)\n",
      "Training QB model for passing_tds...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  QB passing_tds model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=1.2, Median=1.0, StdDev=1.2\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 3/17: QB - completions\n",
      "================================================================================\n",
      "📊 Training on 3,384 records (3,384 valid)\n",
      "Training QB model for completions...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  QB completions model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=18.2, Median=20.0, StdDev=9.2\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 4/17: QB - passing_interceptions\n",
      "================================================================================\n",
      "📊 Training on 3,384 records (3,384 valid)\n",
      "Training QB model for passing_interceptions...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  QB passing_interceptions model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=0.6, Median=0.0, StdDev=0.8\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 5/17: QB - rushing_yards\n",
      "================================================================================\n",
      "📊 Training on 3,384 records (3,384 valid)\n",
      "Training QB model for rushing_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  QB rushing_yards model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=15.3, Median=7.0, StdDev=21.1\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 6/17: RB - rushing_yards\n",
      "================================================================================\n",
      "📊 Training on 7,883 records (7,883 valid)\n",
      "Training RB model for rushing_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  RB rushing_yards model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=33.3, Median=22.0, StdDev=35.9\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 7/17: RB - rushing_tds\n",
      "================================================================================\n",
      "📊 Training on 7,883 records (7,883 valid)\n",
      "Training RB model for rushing_tds...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  RB rushing_tds model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=0.2, Median=0.0, StdDev=0.5\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 8/17: RB - receptions\n",
      "================================================================================\n",
      "📊 Training on 7,883 records (7,883 valid)\n",
      "Training RB model for receptions...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  RB receptions model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=1.6, Median=1.0, StdDev=1.8\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 9/17: RB - receiving_yards\n",
      "================================================================================\n",
      "📊 Training on 7,883 records (7,883 valid)\n",
      "Training RB model for receiving_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  RB receiving_yards model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=12.1, Median=6.0, StdDev=17.0\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 10/17: RB - receiving_tds\n",
      "================================================================================\n",
      "📊 Training on 7,883 records (7,883 valid)\n",
      "Training RB model for receiving_tds...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  RB receiving_tds model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=0.1, Median=0.0, StdDev=0.3\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 11/17: WR - receiving_yards\n",
      "================================================================================\n",
      "📊 Training on 12,670 records (12,670 valid)\n",
      "Training WR model for receiving_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  WR receiving_yards model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=34.4, Median=23.0, StdDev=36.8\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 12/17: WR - receptions\n",
      "================================================================================\n",
      "📊 Training on 12,670 records (12,670 valid)\n",
      "Training WR model for receptions...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  WR receptions model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=2.7, Median=2.0, StdDev=2.6\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 13/17: WR - receiving_tds\n",
      "================================================================================\n",
      "📊 Training on 12,670 records (12,670 valid)\n",
      "Training WR model for receiving_tds...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  WR receiving_tds model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=0.2, Median=0.0, StdDev=0.5\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 14/17: WR - targets\n",
      "================================================================================\n",
      "📊 Training on 12,670 records (12,670 valid)\n",
      "Training WR model for targets...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  WR targets model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=4.3, Median=4.0, StdDev=3.6\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 15/17: TE - receiving_yards\n",
      "================================================================================\n",
      "📊 Training on 6,272 records (6,272 valid)\n",
      "Training TE model for receiving_yards...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  TE receiving_yards model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=23.0, Median=15.0, StdDev=25.9\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 16/17: TE - receptions\n",
      "================================================================================\n",
      "📊 Training on 6,272 records (6,272 valid)\n",
      "Training TE model for receptions...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  TE receptions model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=2.2, Median=2.0, StdDev=2.1\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "🏈 Model 17/17: TE - receiving_tds\n",
      "================================================================================\n",
      "📊 Training on 6,272 records (6,272 valid)\n",
      "Training TE model for receiving_tds...\n",
      "  Training quantile 0.1...\n",
      "  Training quantile 0.5...\n",
      "  Training quantile 0.9...\n",
      "  TE receiving_tds model trained successfully!\n",
      "✅ SUCCESS!\n",
      "   📈 Data stats: Mean=0.2, Median=0.0, StdDev=0.4\n",
      "   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\n",
      "\n",
      "================================================================================\n",
      "📊 TRAINING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Train ALL models for complete betting coverage\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE MODEL SUITE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Define all position-stat combinations to train\n",
    "all_model_configs = [\n",
    "    # QB models\n",
    "    ('QB', 'passing_yards'),\n",
    "    ('QB', 'passing_tds'),\n",
    "    ('QB', 'completions'),\n",
    "    ('QB', 'passing_interceptions'),\n",
    "    ('QB', 'rushing_yards'),\n",
    "    \n",
    "    # RB models  \n",
    "    ('RB', 'rushing_yards'),\n",
    "    ('RB', 'rushing_tds'),\n",
    "    ('RB', 'receptions'),\n",
    "    ('RB', 'receiving_yards'),\n",
    "    ('RB', 'receiving_tds'),\n",
    "    \n",
    "    # WR models\n",
    "    ('WR', 'receiving_yards'),\n",
    "    ('WR', 'receptions'),\n",
    "    ('WR', 'receiving_tds'),\n",
    "    ('WR', 'targets'),\n",
    "    \n",
    "    # TE models\n",
    "    ('TE', 'receiving_yards'),\n",
    "    ('TE', 'receptions'),\n",
    "    ('TE', 'receiving_tds'),\n",
    "]\n",
    "\n",
    "print(f\"📋 Configured to train {len(all_model_configs)} models\")\n",
    "print(f\"🔄 Training with quantiles: [0.1, 0.5, 0.9]\")\n",
    "print(f\"💾 Each model will be saved for production use\\n\")\n",
    "\n",
    "# Track training results\n",
    "training_results = []\n",
    "start_time = pd.Timestamp.now()\n",
    "\n",
    "# Train each model\n",
    "for idx, (position, stat) in enumerate(all_model_configs, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🏈 Model {idx}/{len(all_model_configs)}: {position} - {stat}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get position data\n",
    "    pos_data = processed_data[processed_data['position'] == position].copy()\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(pos_data) < 100:\n",
    "        print(f\"⚠️  SKIPPED: Insufficient data ({len(pos_data)} records, need at least 100)\")\n",
    "        training_results.append({\n",
    "            'position': position,\n",
    "            'stat': stat,\n",
    "            'status': 'SKIPPED',\n",
    "            'reason': 'Insufficient data',\n",
    "            'records': len(pos_data),\n",
    "            'models_trained': 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Check if stat column exists and has data\n",
    "    if stat not in pos_data.columns:\n",
    "        print(f\"⚠️  SKIPPED: Column '{stat}' not found in data\")\n",
    "        training_results.append({\n",
    "            'position': position,\n",
    "            'stat': stat,\n",
    "            'status': 'SKIPPED',\n",
    "            'reason': 'Column not found',\n",
    "            'records': len(pos_data),\n",
    "            'models_trained': 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Check for non-null values\n",
    "    valid_records = pos_data[stat].notna().sum()\n",
    "    if valid_records < 50:\n",
    "        print(f\"⚠️  SKIPPED: Insufficient valid data ({valid_records} non-null records)\")\n",
    "        training_results.append({\n",
    "            'position': position,\n",
    "            'stat': stat,\n",
    "            'status': 'SKIPPED',\n",
    "            'reason': 'Too few valid records',\n",
    "            'records': valid_records,\n",
    "            'models_trained': 0\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        print(f\"📊 Training on {len(pos_data):,} records ({valid_records:,} valid)\")\n",
    "        \n",
    "        model = model_trainer.train_position_model(pos_data, position, stat)\n",
    "        \n",
    "        if model and len(model) == 3:\n",
    "            # Get some stats about the data\n",
    "            stat_mean = pos_data[stat].mean()\n",
    "            stat_median = pos_data[stat].median()\n",
    "            stat_std = pos_data[stat].std()\n",
    "            \n",
    "            print(f\"✅ SUCCESS!\")\n",
    "            print(f\"   📈 Data stats: Mean={stat_mean:.1f}, Median={stat_median:.1f}, StdDev={stat_std:.1f}\")\n",
    "            print(f\"   🎯 Trained 3 quantile models (10th, 50th, 90th percentiles)\")\n",
    "            \n",
    "            training_results.append({\n",
    "                'position': position,\n",
    "                'stat': stat,\n",
    "                'status': 'SUCCESS',\n",
    "                'records': len(pos_data),\n",
    "                'valid_records': valid_records,\n",
    "                'mean': stat_mean,\n",
    "                'median': stat_median,\n",
    "                'std': stat_std,\n",
    "                'models_trained': 3,\n",
    "                'reason': None\n",
    "            })\n",
    "        else:\n",
    "            print(f\"❌ FAILED: Training function did not return expected models\")\n",
    "            training_results.append({\n",
    "                'position': position,\n",
    "                'stat': stat,\n",
    "                'status': 'FAILED',\n",
    "                'reason': 'No models returned',\n",
    "                'records': len(pos_data),\n",
    "                'models_trained': 0\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: {str(e)}\")\n",
    "        training_results.append({\n",
    "            'position': position,\n",
    "            'stat': stat,\n",
    "            'status': 'ERROR',\n",
    "            'reason': str(e)[:100],\n",
    "            'records': len(pos_data),\n",
    "            'models_trained': 0\n",
    "        })\n",
    "\n",
    "# Calculate training time\n",
    "end_time = pd.Timestamp.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2842aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️  Total Training Time: 0.2 minutes (13 seconds)\n",
      "📦 Total Models Configured: 17\n",
      "\n",
      "📊 Training Results by Status:\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ SUCCESS      : 17 models (100.0%)\n",
      "\n",
      "🎯 Total Quantile Models Trained: 51 (each position-stat has 3 quantiles)\n",
      "\n",
      "================================================================================\n",
      "✅ SUCCESSFULLY TRAINED MODELS (17 position-stat combinations)\n",
      "================================================================================\n",
      "\n",
      "QB Models (5):\n",
      "   ✅ passing_yards             | Records: 3,384 | Mean:  199.4 | Median:  214.0\n",
      "   ✅ passing_tds               | Records: 3,384 | Mean:    1.2 | Median:    1.0\n",
      "   ✅ completions               | Records: 3,384 | Mean:   18.2 | Median:   20.0\n",
      "   ✅ passing_interceptions     | Records: 3,384 | Mean:    0.6 | Median:    0.0\n",
      "   ✅ rushing_yards             | Records: 3,384 | Mean:   15.3 | Median:    7.0\n",
      "\n",
      "RB Models (5):\n",
      "   ✅ rushing_yards             | Records: 7,883 | Mean:   33.3 | Median:   22.0\n",
      "   ✅ rushing_tds               | Records: 7,883 | Mean:    0.2 | Median:    0.0\n",
      "   ✅ receptions                | Records: 7,883 | Mean:    1.6 | Median:    1.0\n",
      "   ✅ receiving_yards           | Records: 7,883 | Mean:   12.1 | Median:    6.0\n",
      "   ✅ receiving_tds             | Records: 7,883 | Mean:    0.1 | Median:    0.0\n",
      "\n",
      "WR Models (4):\n",
      "   ✅ receiving_yards           | Records: 12,670 | Mean:   34.4 | Median:   23.0\n",
      "   ✅ receptions                | Records: 12,670 | Mean:    2.7 | Median:    2.0\n",
      "   ✅ receiving_tds             | Records: 12,670 | Mean:    0.2 | Median:    0.0\n",
      "   ✅ targets                   | Records: 12,670 | Mean:    4.3 | Median:    4.0\n",
      "\n",
      "TE Models (3):\n",
      "   ✅ receiving_yards           | Records: 6,272 | Mean:   23.0 | Median:   15.0\n",
      "   ✅ receptions                | Records: 6,272 | Mean:    2.2 | Median:    2.0\n",
      "   ✅ receiving_tds             | Records: 6,272 | Mean:    0.2 | Median:    0.0\n",
      "\n",
      "================================================================================\n",
      "📦 MODEL INVENTORY - Ready for Production\n",
      "================================================================================\n",
      "Total trained model keys: 17\n",
      "Models stored in memory:\n",
      "   ✅ QB_completions                 | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ QB_passing_interceptions       | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ QB_passing_tds                 | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ QB_passing_yards               | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ QB_rushing_yards               | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ RB_receiving_tds               | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ RB_receiving_yards             | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ RB_receptions                  | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ RB_rushing_tds                 | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ RB_rushing_yards               | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ TE_receiving_tds               | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ TE_receiving_yards             | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ TE_receptions                  | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ WR_receiving_tds               | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ WR_receiving_yards             | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ WR_receptions                  | Quantiles: [0.1, 0.5, 0.9]\n",
      "   ✅ WR_targets                     | Quantiles: [0.1, 0.5, 0.9]\n",
      "\n",
      "================================================================================\n",
      "🎉 MODEL TRAINING COMPLETE - Ready for Phase 1 Export!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Display comprehensive training summary\n",
    "# ============================================================================\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(training_results)\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\n⏱️  Total Training Time: {duration/60:.1f} minutes ({duration:.0f} seconds)\")\n",
    "print(f\"📦 Total Models Configured: {len(all_model_configs)}\")\n",
    "\n",
    "# Count by status\n",
    "status_counts = results_df['status'].value_counts()\n",
    "print(f\"\\n📊 Training Results by Status:\")\n",
    "print(\"-\" * 80)\n",
    "for status, count in status_counts.items():\n",
    "    percentage = (count / len(results_df) * 100)\n",
    "    if status == 'SUCCESS':\n",
    "        print(f\"   ✅ {status:12} : {count:2} models ({percentage:5.1f}%)\")\n",
    "    elif status == 'SKIPPED':\n",
    "        print(f\"   ⚠️  {status:12} : {count:2} models ({percentage:5.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ❌ {status:12} : {count:2} models ({percentage:5.1f}%)\")\n",
    "\n",
    "# Calculate total quantile models\n",
    "total_quantile_models = results_df['models_trained'].sum()\n",
    "print(f\"\\n🎯 Total Quantile Models Trained: {total_quantile_models} (each position-stat has 3 quantiles)\")\n",
    "\n",
    "# Display successful models\n",
    "success_df = results_df[results_df['status'] == 'SUCCESS'].copy()\n",
    "if len(success_df) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✅ SUCCESSFULLY TRAINED MODELS ({len(success_df)} position-stat combinations)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Group by position\n",
    "    for position in ['QB', 'RB', 'WR', 'TE']:\n",
    "        pos_models = success_df[success_df['position'] == position]\n",
    "        if len(pos_models) > 0:\n",
    "            print(f\"\\n{position} Models ({len(pos_models)}):\")\n",
    "            for _, row in pos_models.iterrows():\n",
    "                print(f\"   ✅ {row['stat']:25} | Records: {row['records']:5,} | \"\n",
    "                      f\"Mean: {row['mean']:6.1f} | Median: {row['median']:6.1f}\")\n",
    "\n",
    "# Display failed/skipped models\n",
    "failed_df = results_df[results_df['status'].isin(['FAILED', 'SKIPPED', 'ERROR'])].copy()\n",
    "if len(failed_df) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"⚠️  FAILED/SKIPPED MODELS ({len(failed_df)})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for _, row in failed_df.iterrows():\n",
    "        print(f\"   {row['status']:8} | {row['position']:3} - {row['stat']:25} | Reason: {row['reason']}\")\n",
    "\n",
    "# Display model inventory\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"📦 MODEL INVENTORY - Ready for Production\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total trained model keys: {len(model_trainer.models)}\")\n",
    "print(f\"Models stored in memory:\")\n",
    "for model_key in sorted(model_trainer.models.keys()):\n",
    "    quantiles = list(model_trainer.models[model_key].keys())\n",
    "    print(f\"   ✅ {model_key:30} | Quantiles: {quantiles}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"🎉 MODEL TRAINING COMPLETE - Ready for Phase 1 Export!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4503df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: EXPORTING MODELS TO DISK\n",
      "================================================================================\n",
      "\n",
      "📁 Created export directory: c:\\Users\\tonyg\\OneDrive\\Desktop\\University Files\\Fall 2025\\SWE\\Main Project\\hedge-your-bets\\hedge-your-bets\\machine_learning\\training_code\\..\\saved_models\n",
      "\n",
      "STEP 1: Saving individual quantile models...\n",
      "--------------------------------------------------------------------------------\n",
      "✅ Saved: QB_passing_yards_q10.joblib              ( 274.4 KB)\n",
      "✅ Saved: QB_passing_yards_q50.joblib              ( 274.9 KB)\n",
      "✅ Saved: QB_passing_yards_q90.joblib              ( 256.4 KB)\n",
      "✅ Saved: RB_rushing_yards_q10.joblib              ( 278.3 KB)\n",
      "✅ Saved: RB_rushing_yards_q50.joblib              ( 278.7 KB)\n",
      "✅ Saved: RB_rushing_yards_q90.joblib              ( 277.0 KB)\n",
      "✅ Saved: WR_receiving_yards_q10.joblib            ( 266.4 KB)\n",
      "✅ Saved: WR_receiving_yards_q50.joblib            ( 279.8 KB)\n",
      "✅ Saved: WR_receiving_yards_q90.joblib            ( 277.2 KB)\n",
      "✅ Saved: TE_receiving_yards_q10.joblib            ( 271.6 KB)\n",
      "✅ Saved: TE_receiving_yards_q50.joblib            ( 277.7 KB)\n",
      "✅ Saved: TE_receiving_yards_q90.joblib            ( 276.6 KB)\n",
      "✅ Saved: QB_passing_tds_q10.joblib                ( 273.1 KB)\n",
      "✅ Saved: QB_passing_tds_q50.joblib                ( 280.3 KB)\n",
      "✅ Saved: QB_passing_tds_q90.joblib                ( 279.6 KB)\n",
      "✅ Saved: QB_completions_q10.joblib                ( 275.7 KB)\n",
      "✅ Saved: QB_completions_q50.joblib                ( 278.6 KB)\n",
      "✅ Saved: QB_completions_q90.joblib                ( 258.4 KB)\n",
      "✅ Saved: QB_passing_interceptions_q10.joblib      ( 251.7 KB)\n",
      "✅ Saved: QB_passing_interceptions_q50.joblib      ( 276.0 KB)\n",
      "✅ Saved: QB_passing_interceptions_q90.joblib      ( 282.5 KB)\n",
      "✅ Saved: QB_rushing_yards_q10.joblib              ( 281.5 KB)\n",
      "✅ Saved: QB_rushing_yards_q50.joblib              ( 276.5 KB)\n",
      "✅ Saved: QB_rushing_yards_q90.joblib              ( 277.8 KB)\n",
      "✅ Saved: RB_rushing_tds_q10.joblib                ( 245.2 KB)\n",
      "✅ Saved: RB_rushing_tds_q50.joblib                ( 274.6 KB)\n",
      "✅ Saved: RB_rushing_tds_q90.joblib                ( 278.6 KB)\n",
      "✅ Saved: RB_receptions_q10.joblib                 ( 272.5 KB)\n",
      "✅ Saved: RB_receptions_q50.joblib                 ( 283.5 KB)\n",
      "✅ Saved: RB_receptions_q90.joblib                 ( 281.6 KB)\n",
      "✅ Saved: RB_receiving_yards_q10.joblib            ( 275.9 KB)\n",
      "✅ Saved: RB_receiving_yards_q50.joblib            ( 280.7 KB)\n",
      "✅ Saved: RB_receiving_yards_q90.joblib            ( 278.2 KB)\n",
      "✅ Saved: RB_receiving_tds_q10.joblib              ( 219.1 KB)\n",
      "✅ Saved: RB_receiving_tds_q50.joblib              ( 258.8 KB)\n",
      "✅ Saved: RB_receiving_tds_q90.joblib              ( 270.8 KB)\n",
      "✅ Saved: WR_receptions_q10.joblib                 ( 270.8 KB)\n",
      "✅ Saved: WR_receptions_q50.joblib                 ( 283.9 KB)\n",
      "✅ Saved: WR_receptions_q90.joblib                 ( 280.3 KB)\n",
      "✅ Saved: WR_receiving_tds_q10.joblib              ( 231.7 KB)\n",
      "✅ Saved: WR_receiving_tds_q50.joblib              ( 272.9 KB)\n",
      "✅ Saved: WR_receiving_tds_q90.joblib              ( 278.3 KB)\n",
      "✅ Saved: WR_targets_q10.joblib                    ( 278.8 KB)\n",
      "✅ Saved: WR_targets_q50.joblib                    ( 285.1 KB)\n",
      "✅ Saved: WR_targets_q90.joblib                    ( 281.3 KB)\n",
      "✅ Saved: TE_receptions_q10.joblib                 ( 271.3 KB)\n",
      "✅ Saved: TE_receptions_q50.joblib                 ( 281.9 KB)\n",
      "✅ Saved: TE_receptions_q90.joblib                 ( 279.4 KB)\n",
      "✅ Saved: TE_receiving_tds_q10.joblib              ( 236.7 KB)\n",
      "✅ Saved: TE_receiving_tds_q50.joblib              ( 274.8 KB)\n",
      "✅ Saved: TE_receiving_tds_q90.joblib              ( 262.6 KB)\n",
      "\n",
      "✅ Saved 51 quantile model files\n",
      "\n",
      "STEP 2: Saving feature column metadata...\n",
      "--------------------------------------------------------------------------------\n",
      "✅ Saved: feature_columns.joblib (6.5 KB)\n",
      "   Contains feature lists for 17 models\n",
      "\n",
      "STEP 3: Saving model metadata...\n",
      "--------------------------------------------------------------------------------\n",
      "✅ Saved: model_metadata.joblib (3.2 KB)\n",
      "   Contains metadata for 17 models\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PHASE 1: EXPORT MODELS - Save all trained models to disk\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: EXPORTING MODELS TO DISK\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create directory for saved models\n",
    "model_dir = Path('../saved_models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"📁 Created export directory: {model_dir.absolute()}\\n\")\n",
    "\n",
    "# Step 1: Save all quantile models\n",
    "print(\"STEP 1: Saving individual quantile models...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "for model_key, quantile_models in model_trainer.models.items():\n",
    "    # model_key is like 'QB_passing_yards'\n",
    "    position, stat = model_key.split('_', 1)\n",
    "    \n",
    "    # Save each quantile model (0.1, 0.5, 0.9)\n",
    "    for quantile, model in quantile_models.items():\n",
    "        # Create filename like: QB_passing_yards_q10.joblib\n",
    "        filename = f\"{model_key}_q{int(quantile*100)}.joblib\"\n",
    "        filepath = model_dir / filename\n",
    "        \n",
    "        # Save the model using joblib\n",
    "        joblib.dump(model, filepath)\n",
    "        \n",
    "        # Get file size\n",
    "        file_size_kb = filepath.stat().st_size / 1024\n",
    "        \n",
    "        saved_files.append({\n",
    "            'position': position,\n",
    "            'stat': stat.replace('_', ' '),\n",
    "            'quantile': quantile,\n",
    "            'filename': filename,\n",
    "            'size_kb': file_size_kb\n",
    "        })\n",
    "        \n",
    "        print(f\"✅ Saved: {filename:40} ({file_size_kb:6.1f} KB)\")\n",
    "\n",
    "print(f\"\\n✅ Saved {len(saved_files)} quantile model files\")\n",
    "\n",
    "# Step 2: Save feature columns\n",
    "print(f\"\\nSTEP 2: Saving feature column metadata...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "feature_cols_path = model_dir / 'feature_columns.joblib'\n",
    "joblib.dump(model_trainer.feature_columns, feature_cols_path)\n",
    "file_size_kb = feature_cols_path.stat().st_size / 1024\n",
    "print(f\"✅ Saved: feature_columns.joblib ({file_size_kb:.1f} KB)\")\n",
    "print(f\"   Contains feature lists for {len(model_trainer.feature_columns)} models\")\n",
    "\n",
    "# Step 3: Save model metadata\n",
    "print(f\"\\nSTEP 3: Saving model metadata...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "metadata = {\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'data_years': '2020-2024',\n",
    "    'total_records_processed': len(processed_data),\n",
    "    'models_trained': list(model_trainer.models.keys()),\n",
    "    'quantiles': [0.1, 0.5, 0.9],\n",
    "    'positions': ['QB', 'RB', 'WR', 'TE'],\n",
    "    'model_details': {},\n",
    "    'training_summary': results_df.to_dict('records')\n",
    "}\n",
    "\n",
    "# Add details for each successful model\n",
    "for _, row in success_df.iterrows():\n",
    "    key = f\"{row['position']}_{row['stat']}\"\n",
    "    metadata['model_details'][key] = {\n",
    "        'position': row['position'],\n",
    "        'stat': row['stat'],\n",
    "        'training_records': int(row['records']),\n",
    "        'valid_records': int(row['valid_records']),\n",
    "        'mean': float(row['mean']),\n",
    "        'median': float(row['median']),\n",
    "        'std': float(row['std']),\n",
    "        'quantiles_trained': 3\n",
    "    }\n",
    "\n",
    "metadata_path = model_dir / 'model_metadata.joblib'\n",
    "joblib.dump(metadata, metadata_path)\n",
    "file_size_kb = metadata_path.stat().st_size / 1024\n",
    "print(f\"✅ Saved: model_metadata.joblib ({file_size_kb:.1f} KB)\")\n",
    "print(f\"   Contains metadata for {len(metadata['model_details'])} models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f353dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 4: Exporting preprocessing code...\n",
      "--------------------------------------------------------------------------------\n",
      "✅ Saved: inference_preprocessing.py (4.5 KB)\n",
      "   Contains InferencePreprocessor class and utility mappings\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Export preprocessing code as Python module\n",
    "print(f\"\\nSTEP 4: Exporting preprocessing code...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "preprocessing_code = '''\"\"\"\n",
    "Preprocessing utilities for Hedge Your Bets ML inference\n",
    "Auto-generated from model_training.ipynb\n",
    "Training Date: {training_date}\n",
    "Data: 2020-2024 NFL player and team stats\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class InferencePreprocessor:\n",
    "    \"\"\"Preprocessing pipeline for making predictions on new betting scenarios.\"\"\"\n",
    "    \n",
    "    def __init__(self, datasets_dir='../datasets'):\n",
    "        \"\"\"Initialize with path to datasets directory.\"\"\"\n",
    "        self.datasets_dir = Path(datasets_dir)\n",
    "        self.player_weekly_dir = self.datasets_dir / \"player_weekly_stats\"\n",
    "        self.team_weekly_dir = self.datasets_dir / \"team_season_stats\"\n",
    "        \n",
    "    def add_temporal_features(self, df):\n",
    "        \"\"\"Add temporal features like season progression.\"\"\"\n",
    "        df = df.copy()\n",
    "        df['season_progression'] = df['week'] / 18.0\n",
    "        df['is_playoff'] = (df['season_type'] == 'POST').astype(int)\n",
    "        df['is_home'] = 0  # Placeholder - would need schedule data for actual home/away\n",
    "        return df\n",
    "    \n",
    "    def create_rolling_features(self, df, stat_cols, windows=[3, 5]):\n",
    "        \"\"\"Create rolling averages for key statistics.\"\"\"\n",
    "        df = df.sort_values(['player_id', 'season', 'week'])\n",
    "        \n",
    "        for window in windows:\n",
    "            for stat in stat_cols:\n",
    "                if stat in df.columns:\n",
    "                    # Rolling mean\n",
    "                    df[f'{{stat}}_avg_{{window}}'] = df.groupby('player_id')[stat].rolling(\n",
    "                        window=window, min_periods=1\n",
    "                    ).mean().reset_index(0, drop=True)\n",
    "                    \n",
    "                    # Rolling std\n",
    "                    df[f'{{stat}}_std_{{window}}'] = df.groupby('player_id')[stat].rolling(\n",
    "                        window=window, min_periods=1\n",
    "                    ).std().reset_index(0, drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def merge_team_context(self, player_df, team_df):\n",
    "        \"\"\"Merge team-level context features.\"\"\"\n",
    "        team_df = team_df.copy()\n",
    "        team_df['team_game_id'] = team_df['season'].astype(str) + '_' + team_df['week'].astype(str) + '_' + team_df['team']\n",
    "        \n",
    "        # Select relevant team features (only numeric ones)\n",
    "        team_features = ['passing_yards', 'rushing_yards', 'receptions', 'targets']\n",
    "        team_subset = team_df[['team_game_id'] + team_features].copy()\n",
    "        \n",
    "        # Rename columns to indicate team context\n",
    "        team_subset.columns = ['team_game_id'] + [f'team_{{col}}' for col in team_features]\n",
    "        \n",
    "        # Create game identifier in player data\n",
    "        player_df['game_id'] = player_df['season'].astype(str) + '_' + player_df['week'].astype(str) + '_' + player_df['team']\n",
    "        \n",
    "        # Merge with player data\n",
    "        player_df = player_df.merge(team_subset, left_on='game_id', right_on='team_game_id', how='left')\n",
    "        \n",
    "        # Drop the team_game_id column\n",
    "        if 'team_game_id' in player_df.columns:\n",
    "            player_df = player_df.drop('team_game_id', axis=1)\n",
    "        \n",
    "        return player_df\n",
    "    \n",
    "    def get_player_recent_games(self, player_id, season, week, num_games=5):\n",
    "        \"\"\"\n",
    "        Get recent game data for a player to calculate rolling features.\n",
    "        This is the main function you'll use for inference.\n",
    "        \"\"\"\n",
    "        # Load recent data (this is simplified - in production you'd have this data ready)\n",
    "        # For now, assumes data is loaded from CSVs\n",
    "        pass  # Implement based on your data loading strategy\n",
    "\n",
    "\n",
    "# Position-specific stat mappings\n",
    "POSITION_STATS = {{\n",
    "    'QB': ['passing_yards', 'passing_tds', 'completions', 'attempts', 'passing_interceptions', 'rushing_yards'],\n",
    "    'RB': ['rushing_yards', 'rushing_tds', 'carries', 'receptions', 'receiving_yards', 'receiving_tds'],\n",
    "    'WR': ['receiving_yards', 'receptions', 'receiving_tds', 'targets'],\n",
    "    'TE': ['receiving_yards', 'receptions', 'receiving_tds']\n",
    "}}\n",
    "\n",
    "# Action name mappings (frontend -> model stat name)\n",
    "ACTION_TO_STAT = {{\n",
    "    'Passing Yards': 'passing_yards',\n",
    "    'Passing TDs': 'passing_tds',\n",
    "    'Completions': 'completions',\n",
    "    'Interceptions': 'passing_interceptions',\n",
    "    'Rushing Yards': 'rushing_yards',\n",
    "    'Rushing TDs': 'rushing_tds',\n",
    "    'Receiving Yards': 'receiving_yards',\n",
    "    'Receiving TDs': 'receiving_tds',\n",
    "    'Receptions': 'receptions',\n",
    "    'Targets': 'targets',\n",
    "    'Touchdowns': 'touchdowns'  # Note: May need position-specific handling\n",
    "}}\n",
    "\n",
    "# Stat to action name (reverse mapping)\n",
    "STAT_TO_ACTION = {{v: k for k, v in ACTION_TO_STAT.items()}}\n",
    "'''.format(training_date=datetime.now().isoformat())\n",
    "\n",
    "# Write to file\n",
    "preprocessing_file = model_dir / 'inference_preprocessing.py'\n",
    "with open(preprocessing_file, 'w') as f:\n",
    "    f.write(preprocessing_code)\n",
    "\n",
    "file_size_kb = preprocessing_file.stat().st_size / 1024\n",
    "print(f\"✅ Saved: inference_preprocessing.py ({file_size_kb:.1f} KB)\")\n",
    "print(f\"   Contains InferencePreprocessor class and utility mappings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e774b19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 5: Testing model loading...\n",
      "--------------------------------------------------------------------------------\n",
      "✅ QB Passing Yards Median             | Loaded & tested | Prediction: 239.3\n",
      "✅ RB Rushing Yards Median             | Loaded & tested | Prediction: 23.9\n",
      "✅ WR Receiving Yards Median           | Loaded & tested | Prediction: 33.9\n",
      "\n",
      "✅ All model load tests passed!\n",
      "\n",
      "Testing metadata loading...\n",
      "✅ Metadata loaded successfully\n",
      "   Training date: 2025-10-19T22:52:11.336151\n",
      "   Models: 17\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Test loading models to verify they work\n",
    "print(f\"\\nSTEP 5: Testing model loading...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Test loading a few models\n",
    "test_models = [\n",
    "    ('QB_passing_yards_q50.joblib', 'QB Passing Yards Median'),\n",
    "    ('RB_rushing_yards_q50.joblib', 'RB Rushing Yards Median'),\n",
    "    ('WR_receiving_yards_q50.joblib', 'WR Receiving Yards Median')\n",
    "]\n",
    "\n",
    "all_tests_passed = True\n",
    "\n",
    "for filename, description in test_models:\n",
    "    filepath = model_dir / filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        try:\n",
    "            # Load the model\n",
    "            loaded_model = joblib.load(filepath)\n",
    "            \n",
    "            # Make a simple test prediction\n",
    "            # Get the position and stat from filename\n",
    "            parts = filename.replace('.joblib', '').split('_')\n",
    "            position = parts[0]\n",
    "            stat = '_'.join(parts[1:-1])  # Everything except position and quantile\n",
    "            \n",
    "            # Get sample data\n",
    "            pos_data = processed_data[processed_data['position'] == position]\n",
    "            if len(pos_data) > 0:\n",
    "                feature_cols = model_trainer.feature_columns[f'{position}_{stat}']\n",
    "                test_features = pos_data[feature_cols].fillna(0).iloc[0:1]\n",
    "                test_prediction = loaded_model.predict(test_features)\n",
    "                \n",
    "                print(f\"✅ {description:35} | Loaded & tested | Prediction: {test_prediction[0]:.1f}\")\n",
    "            else:\n",
    "                print(f\"⚠️  {description:35} | Loaded but no test data\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {description:35} | FAILED: {str(e)[:50]}\")\n",
    "            all_tests_passed = False\n",
    "    else:\n",
    "        print(f\"❌ {description:35} | File not found\")\n",
    "        all_tests_passed = False\n",
    "\n",
    "if all_tests_passed:\n",
    "    print(f\"\\n✅ All model load tests passed!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Some tests failed - check errors above\")\n",
    "\n",
    "# Test loading metadata\n",
    "print(f\"\\nTesting metadata loading...\")\n",
    "try:\n",
    "    loaded_metadata = joblib.load(metadata_path)\n",
    "    print(f\"✅ Metadata loaded successfully\")\n",
    "    print(f\"   Training date: {loaded_metadata['training_date']}\")\n",
    "    print(f\"   Models: {len(loaded_metadata['models_trained'])}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Metadata load failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b00e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🎉 PHASE 1 COMPLETE: MODELS EXPORTED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "📁 Export Directory:\n",
      "   c:\\Users\\tonyg\\OneDrive\\Desktop\\University Files\\Fall 2025\\SWE\\Main Project\\hedge-your-bets\\hedge-your-bets\\machine_learning\\training_code\\..\\saved_models\n",
      "\n",
      "📦 Exported Files (54 total):\n",
      "\n",
      "   Quantile Models: 51\n",
      "      • QB_completions_q10.joblib                ( 275.7 KB)\n",
      "      • QB_completions_q50.joblib                ( 278.6 KB)\n",
      "      • QB_completions_q90.joblib                ( 258.4 KB)\n",
      "      • QB_passing_interceptions_q10.joblib      ( 251.7 KB)\n",
      "      • QB_passing_interceptions_q50.joblib      ( 276.0 KB)\n",
      "      ... and 46 more model files\n",
      "\n",
      "   Metadata Files: 2\n",
      "      • feature_columns.joblib                   (   6.5 KB)\n",
      "      • model_metadata.joblib                    (   3.2 KB)\n",
      "\n",
      "   Python Modules: 1\n",
      "      • inference_preprocessing.py               (   4.5 KB)\n",
      "\n",
      "   💾 Total Size: 13.56 MB (13884.3 KB)\n",
      "\n",
      "================================================================================\n",
      "🏈 MODEL COVERAGE - What Can Be Predicted\n",
      "================================================================================\n",
      "\n",
      "QB  Position (5 stats):\n",
      "   ✅ Completions\n",
      "   ✅ Passing Interceptions\n",
      "   ✅ Passing Tds\n",
      "   ✅ Passing Yards\n",
      "   ✅ Rushing Yards\n",
      "\n",
      "RB  Position (5 stats):\n",
      "   ✅ Receiving Tds\n",
      "   ✅ Receiving Yards\n",
      "   ✅ Receptions\n",
      "   ✅ Rushing Tds\n",
      "   ✅ Rushing Yards\n",
      "\n",
      "WR  Position (4 stats):\n",
      "   ✅ Receiving Tds\n",
      "   ✅ Receiving Yards\n",
      "   ✅ Receptions\n",
      "   ✅ Targets\n",
      "\n",
      "TE  Position (3 stats):\n",
      "   ✅ Receiving Tds\n",
      "   ✅ Receiving Yards\n",
      "   ✅ Receptions\n",
      "\n",
      "================================================================================\n",
      "📋 NEXT STEPS\n",
      "================================================================================\n",
      "\n",
      "1. ✅ DONE: Train all models for betting scenarios\n",
      "2. ✅ DONE: Export models to disk with joblib\n",
      "3. ✅ DONE: Save feature metadata and preprocessing code\n",
      "\n",
      "READY FOR PHASE 2:\n",
      "4. 🔜 Copy saved_models/ folder to Django backend\n",
      "5. 🔜 Create ML service module (model_loader.py, predictor.py)\n",
      "6. 🔜 Add prediction API endpoint\n",
      "7. 🔜 Connect frontend to backend\n",
      "8. 🔜 Test end-to-end prediction pipeline\n",
      "\n",
      "Your models are now ready for production deployment! 🚀\n",
      "\n",
      "================================================================================\n",
      "Notebook execution complete: 2025-10-19 22:53:57\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY: Complete Export Report\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 PHASE 1 COMPLETE: MODELS EXPORTED SUCCESSFULLY!\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Directory summary\n",
    "print(\"📁 Export Directory:\")\n",
    "print(f\"   {model_dir.absolute()}\\n\")\n",
    "\n",
    "# List all files\n",
    "all_files = list(model_dir.glob('*'))\n",
    "print(f\"📦 Exported Files ({len(all_files)} total):\\n\")\n",
    "\n",
    "# Group files by type\n",
    "model_files = [f for f in all_files if f.suffix == '.joblib' and 'q' in f.stem]\n",
    "metadata_files = [f for f in all_files if f.suffix == '.joblib' and 'q' not in f.stem]\n",
    "python_files = [f for f in all_files if f.suffix == '.py']\n",
    "\n",
    "print(f\"   Quantile Models: {len(model_files)}\")\n",
    "for f in sorted(model_files)[:5]:  # Show first 5\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"      • {f.name:40} ({size_kb:6.1f} KB)\")\n",
    "if len(model_files) > 5:\n",
    "    print(f\"      ... and {len(model_files) - 5} more model files\")\n",
    "\n",
    "print(f\"\\n   Metadata Files: {len(metadata_files)}\")\n",
    "for f in sorted(metadata_files):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"      • {f.name:40} ({size_kb:6.1f} KB)\")\n",
    "\n",
    "print(f\"\\n   Python Modules: {len(python_files)}\")\n",
    "for f in sorted(python_files):\n",
    "    size_kb = f.stat().st_size / 1024\n",
    "    print(f\"      • {f.name:40} ({size_kb:6.1f} KB)\")\n",
    "\n",
    "# Calculate total size\n",
    "total_size_kb = sum(f.stat().st_size for f in all_files) / 1024\n",
    "total_size_mb = total_size_kb / 1024\n",
    "print(f\"\\n   💾 Total Size: {total_size_mb:.2f} MB ({total_size_kb:.1f} KB)\")\n",
    "\n",
    "# Model coverage summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"🏈 MODEL COVERAGE - What Can Be Predicted\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "coverage_summary = {}\n",
    "for model_key in sorted(model_trainer.models.keys()):\n",
    "    position, stat = model_key.split('_', 1)\n",
    "    if position not in coverage_summary:\n",
    "        coverage_summary[position] = []\n",
    "    coverage_summary[position].append(stat.replace('_', ' ').title())\n",
    "\n",
    "for position in ['QB', 'RB', 'WR', 'TE']:\n",
    "    if position in coverage_summary:\n",
    "        stats = coverage_summary[position]\n",
    "        print(f\"{position:3} Position ({len(stats)} stats):\")\n",
    "        for stat in sorted(stats):\n",
    "            print(f\"   ✅ {stat}\")\n",
    "        print()\n",
    "\n",
    "# Next steps\n",
    "print(\"=\"*80)\n",
    "print(\"📋 NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. ✅ DONE: Train all models for betting scenarios\n",
    "2. ✅ DONE: Export models to disk with joblib\n",
    "3. ✅ DONE: Save feature metadata and preprocessing code\n",
    "\n",
    "READY FOR PHASE 2:\n",
    "4. 🔜 Copy saved_models/ folder to Django backend\n",
    "5. 🔜 Create ML service module (model_loader.py, predictor.py)\n",
    "6. 🔜 Add prediction API endpoint\n",
    "7. 🔜 Connect frontend to backend\n",
    "8. 🔜 Test end-to-end prediction pipeline\n",
    "\n",
    "Your models are now ready for production deployment! 🚀\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Notebook execution complete: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aaf12f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Created README.md in saved_models directory\n",
      "   Path: c:\\Users\\tonyg\\OneDrive\\Desktop\\University Files\\Fall 2025\\SWE\\Main Project\\hedge-your-bets\\hedge-your-bets\\machine_learning\\training_code\\..\\saved_models\\README.md\n"
     ]
    }
   ],
   "source": [
    "# Create README for saved_models directory\n",
    "readme_content = f\"\"\"# Hedge Your Bets - Trained ML Models\n",
    "\n",
    "**Training Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Data Range:** 2020-2024 NFL Season Data\n",
    "**Total Records:** {len(processed_data):,}\n",
    "\n",
    "## 📦 Directory Contents\n",
    "\n",
    "### Model Files (*.joblib)\n",
    "Quantile regression models trained on NFL player statistics. Each position-stat combination has 3 quantile models:\n",
    "- `q10` = 10th percentile (pessimistic prediction)\n",
    "- `q50` = 50th percentile (median/most likely prediction)\n",
    "- `q90` = 90th percentile (optimistic prediction)\n",
    "\n",
    "**Naming Convention:** `{{POSITION}}_{{STAT}}_q{{QUANTILE}}.joblib`\n",
    "\n",
    "Example: `QB_passing_yards_q50.joblib` is the median passing yards model for quarterbacks.\n",
    "\n",
    "### Metadata Files\n",
    "- `model_metadata.joblib` - Complete training metadata, stats, and model performance\n",
    "- `feature_columns.joblib` - Feature lists for each model (CRITICAL for predictions)\n",
    "\n",
    "### Python Modules\n",
    "- `inference_preprocessing.py` - Preprocessing utilities for making predictions\n",
    "\n",
    "## 🏈 Model Coverage\n",
    "\n",
    "### Quarterback (QB)\n",
    "{chr(10).join([f'- {stat.replace(\"_\", \" \").title()}' for stat in coverage_summary.get('QB', [])])}\n",
    "\n",
    "### Running Back (RB)\n",
    "{chr(10).join([f'- {stat.replace(\"_\", \" \").title()}' for stat in coverage_summary.get('RB', [])])}\n",
    "\n",
    "### Wide Receiver (WR)\n",
    "{chr(10).join([f'- {stat.replace(\"_\", \" \").title()}' for stat in coverage_summary.get('WR', [])])}\n",
    "\n",
    "### Tight End (TE)\n",
    "{chr(10).join([f'- {stat.replace(\"_\", \" \").title()}' for stat in coverage_summary.get('TE', [])])}\n",
    "\n",
    "## 📊 Model Statistics\n",
    "\n",
    "- **Algorithm:** LightGBM Quantile Regression\n",
    "- **Training Method:** Per-position models with position-specific features\n",
    "- **Features:** 20-27 features per position including:\n",
    "  - Rolling averages (3-game and 5-game windows)\n",
    "  - Team context statistics\n",
    "  - Temporal features (season progression, playoff indicator)\n",
    "  \n",
    "## 🚀 Usage in Production\n",
    "\n",
    "### Loading a Model\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Load a model\n",
    "model = joblib.load('QB_passing_yards_q50.joblib')\n",
    "\n",
    "# Load feature columns (REQUIRED!)\n",
    "feature_columns = joblib.load('feature_columns.joblib')\n",
    "features = feature_columns['QB_passing_yards']\n",
    "\n",
    "# Make prediction (ensure features match exactly)\n",
    "prediction = model.predict(your_features_dataframe[features])\n",
    "```\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "1. **Feature Order Matters:** Always use the exact feature list from `feature_columns.joblib`\n",
    "2. **Rolling Features Required:** Models need player's last 3-5 games to calculate rolling averages\n",
    "3. **Team Context:** Models expect team-level statistics for the game\n",
    "4. **Data Preprocessing:** Use `inference_preprocessing.py` to prepare features correctly\n",
    "\n",
    "## ⚠️ Important Considerations\n",
    "\n",
    "- Models trained on 2020-2024 data - performance may degrade with future seasons\n",
    "- Models assume similar stat distributions - major rule changes could affect accuracy\n",
    "- Always validate that requested position-stat combinations have trained models\n",
    "- Handle edge cases: rookies (no history), injured players (stale data), etc.\n",
    "\n",
    "---\n",
    "*Generated by model_training.ipynb*\n",
    "*For questions or issues, refer to the training notebook*\n",
    "\"\"\"\n",
    "\n",
    "readme_path = model_dir / 'README.md'\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"\\n✅ Created README.md in saved_models directory\")\n",
    "print(f\"   Path: {readme_path.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36d125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
